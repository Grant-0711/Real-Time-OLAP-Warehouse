# Idea Project

https://github.com/Grant-0711/Real-Time-OLAP-Warehouse/tree/main/gmall2021-parent

# DWD层: 用户行为日志

## 为常用的模块创建工具类

### **FlinkSourceUtil（获取flinkSource）**

一个获取kafka source的静态方法：

​	kafka地址

​	指定消费者组

​	指定消费的topic

返回值是一个FlinkKafkaConsumer<T>，其中需要指定：

​	topic

​	schema：new SimpleStringSchema()

​	properties

properties中需要指定：

​	地址

​	消费者组

​	offset auto.offset.reset：latest/from-beginning

​	隔离级别isolation.level ：read_committed 是读取事务已经提交的

```java
public class FlinkSourceUtil {
    public static FlinkKafkaConsumer<String> getKafkaSource(String groupId, String topic){
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("group.id", groupId);
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("isolation.level", "read_committed");

        return new FlinkKafkaConsumer<String>(
                topic,
                new SimpleStringSchema(),
                props
```



### **Constant（放置常用的常量）**

```java
public class Constant {
    public static final String TOPIC_ODS_LOG = "ods_log";
    public static final String TOPIC_DWD_START_LOG = "dwd_start_log";
    public static final String TOPIC_DWD_PAGE_LOG = "dwd_page_log";
    public static final String TOPIC_DWD_DISPLAY_LOG = "dwd_display_log";
    public static final String TOPIC_ODS_DB = "ods_db";
```



### **BaseAppV1（获取运行环境以及基本配置）**

子类继承该类，实现具体业务

初始化方法创建运行环境，配置相关参数，例如

​	HADOOP_USER_NAME

​	设置精准一次性保证（默认）

​	Checkpoint必须在一分钟内完成，否则就会被抛弃

​	开启在 job 中止后仍然保留的 externalized checkpoints

​	设置状态后端

通过StreamExecutionEnvironment.getExecutionEnvironment()创建运行环境

通过env.addSource(MyKafkaUtil.getKafkaSource(groupId, topic))创建数据流



run方法获取运行环境和数据流，是一个抽象方法，子类重写，实现业务。

```java
public abstract class BaseAppV1 {
    public abstract void run(StreamExecutionEnvironment env, DataStreamSource<String> sourceStream );

    public void init(int port,
                     int p,
                     String ck,
                     String groupId,
                     String topic) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", port);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(p);

        // 设置状态后端
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop162:8020/flink-realtime/ck/" + ck);

        env.enableCheckpointing(3000, CheckpointingMode.EXACTLY_ONCE);

        env.getCheckpointConfig().setCheckpointTimeout(60 * 1000);
        env.getCheckpointConfig()
                .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 具体的业务
        DataStreamSource<String> sourceStream = env
                .addSource(FlinkSourceUtil.getKafkaSource(groupId, topic));

//        sourceStream.print();  // 不同的应用有不同的业务
        run(env, sourceStream);
```

## DWDLogApp具体实现

首先要继承BaseAppV1，重写run方法。由于BaseAppV1中init方法不是静态的，所以要想在main方法中调用init，需要先创建DWDLogApp对象。

在创建对象时传入不同的参数

### 主要任务

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

### 实现思路

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

​	1.考虑数据乱序，因此采用eventTime

​	2.添加窗口

​		每个用户生命周期中的第一个窗口中才会有可能设置为新用户

​		按照eventTime排序，最小的应该是新用户记录，其他就是旧用户

​	3.如何确定第一个窗口

​		使用状态记录



ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

# DWD层: 业务数据

业务数据的来源是由MaxWell从mysql采集得到

但是MaxWell把全部数据统一写入一个Topic, 包括业务数据，也包含维度数据，不利于日后的数据处理

所以DWD业务数据层从Kafka的业务数据ODS层读取数据，经过处理后，将**维度数据保存到Hbase**，将**事实数据写回Kafka**作为业务数据的**DWD层**。

## 主要任务

接受kafka数据并且过滤空值

实现动态分流

​	实现策略是将动态配置方案存入mysql中

​	视图

https://www.processon.com/diagraming/60dcbdc5e0b34d238be0a17e

## 具体实现

### 设计动态配置表

将配置表封装成java实体类

```java
public class TableProcess {
    //动态分流Sink常量
    public static final String SINK_TYPE_HBASE = "hbase";
    public static final String SINK_TYPE_KAFKA = "kafka";
    public static final String SINK_TYPE_CK = "clickhouse";
    //来源表
    private String sourceTable;
    //操作类型 insert,update,delete
    private String operateType;
    //输出类型 hbase kafka
    private String sinkType;
    //输出表(主题)
    private String sinkTable;
    ---略
```

### Flink从kafka读取数据

### Flink从mysql读取配置（CDC）

#### Flink SQL CDC

CDC 全称是 **Change Data Capture** ，捕获变更数据

#### Flink SQL内置debeizum

业界主要有基于查询的 CDC 和基于日志的 CDC 

#### 查询CDC和日志CDC对比（重点）

|                          | 查询CDC                                      | 日志CDC                                |
| ------------------------ | :------------------------------------------- | -------------------------------------- |
| 概念                     | 每次捕获变更都发起查询，全表扫表过滤变更数据 | 读取数据库log保持监控 例如mysql binlog |
| 开源产品                 | sqoop kafka jdbc source                      | canal maxwell debezium                 |
| 执行模式                 | batch                                        | streaming                              |
| 捕获所有变化             | 不可以                                       | 可                                     |
| 低延迟，负载小           | 不可以                                       | 可                                     |
| 不侵入业务（lastupdate） | 不可以                                       | 可                                     |
| 捕获删除和旧的           | 不可以                                       | 可                                     |
| 捕获旧的状态             | 不可以                                       | 可                                     |



Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取**全量数据**和**增量变更数据**的 source 组件

<https://github.com/ververica/flink-cdc-connectors>

#### 代码构建

①读取配置表的数据（flink-sql）

​		获取tableStream运行环境

​		封装一个方法读取配置表的数据

​		创建一个临时表来存配置表的数据

​		将查询到的数据临时表转换为流，流的数据类型用封装的样例类

​		对流数据进行过滤

​		

②对sourceStream数据做一些ETL

​		map ------> filter

```java
                .map(JSON::parseObject)
                .filter(obj ->
                        obj.getString("database") != null
                                && obj.getString("table") != null
                                && obj.getString("type") != null
                                && (obj.getString("type").contains("insert") || obj.get("type").equals("update"))
                                && obj.getString("data") != null
                                && obj.getString("data").length() >2
```



#### 注意事项

##### 读取mysql模式的问题

​	snapshot.mode

​		用来控制debezium如何从mysql读数据

​		参数 initial 是程序一启动则首先把表中所有数据读出（通过mysql查询）

​		基于binlog来监控新增和变化

​		参数 never 只是监控变化，不监控原始数据

​	指定格式

```
debezium.snapshot.mode =  initial or never 
```



##### Maxwell bootstrap

旧的数据需要通过bootstrap的方式读取，导致obj.getString("type")为bootstrap insert 还会包含bootstrap complete，因此在过滤时要考虑这个问题

### 将配置做成广播流与业务数据进行connect，动态控制业务数据的sink方向