# Idea Project

https://github.com/Grant-0711/Real-Time-OLAP-Warehouse/tree/main/gmall2021-parent

# 

# 0 模块创建工具类和函数

### 0 .1**FlinkSourceUtil（获取flinkSource）**

一个获取kafka source的静态方法：

​	kafka地址

​	指定消费者组

​	指定消费的topic

返回值是一个FlinkKafkaConsumer<T>，其中需要指定：

​	topic

​	schema：new SimpleStringSchema()

​	properties

properties中需要指定：

​	地址

​	消费者组

​	offset auto.offset.reset：latest/from-beginning

​	隔离级别isolation.level ：read_committed 是读取事务已经提交的

```java
public class FlinkSourceUtil {
    public static FlinkKafkaConsumer<String> getKafkaSource(String groupId, String topic){
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("group.id", groupId);
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("isolation.level", "read_committed");

        return new FlinkKafkaConsumer<String>(
                topic,
                new SimpleStringSchema(),
                props
```



### **0 .2 Constant（放置常用的常量）**

```java
public class Constant {
    public static final String TOPIC_ODS_LOG = "ods_log";
    public static final String TOPIC_DWD_START_LOG = "dwd_start_log";
    public static final String TOPIC_DWD_PAGE_LOG = "dwd_page_log";
    public static final String TOPIC_DWD_DISPLAY_LOG = "dwd_display_log";
    public static final String TOPIC_ODS_DB = "ods_db";
```



### **0 .3 BaseAppV1（获取运行环境以及基本配置）**

子类继承该类，实现具体业务

初始化方法创建运行环境，配置相关参数，例如

​	HADOOP_USER_NAME

​	设置精准一次性保证（默认）

​	Checkpoint必须在一分钟内完成，否则就会被抛弃

​	开启在 job 中止后仍然保留的 externalized checkpoints

​	设置状态后端

通过StreamExecutionEnvironment.getExecutionEnvironment()创建运行环境

通过env.addSource(MyKafkaUtil.getKafkaSource(groupId, topic))创建数据流



run方法获取运行环境和数据流，是一个抽象方法，子类重写，实现业务。

```java
public abstract class BaseAppV1 {
    public abstract void run(StreamExecutionEnvironment env, DataStreamSource<String> sourceStream );

    public void init(int port,
                     int p,
                     String ck,
                     String groupId,
                     String topic) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", port);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(p);

        // 设置状态后端
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop162:8020/flink-realtime/ck/" + ck);

        env.enableCheckpointing(3000, CheckpointingMode.EXACTLY_ONCE);

        env.getCheckpointConfig().setCheckpointTimeout(60 * 1000);
        env.getCheckpointConfig()
                .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 具体的业务
        DataStreamSource<String> sourceStream = env
                .addSource(FlinkSourceUtil.getKafkaSource(groupId, topic));

//        sourceStream.print();  // 不同的应用有不同的业务
        run(env, sourceStream);
```

### 0 .4 BaseAppV2

与V1的区别是传入多个topic，以可变长参数的形式

#### 操作区别

用一个array存topic

```java
        ArrayList<String> topics = new ArrayList<>(Arrays.asList(otherTopics));
        topics.add(topic);
```

对于array中的每一个topic都调用运行环境的addSource方法

```java
HashMap<String, DataStreamSource<String>> topicAndStreamMap = new HashMap<>();
        for (String t : topics) {
            DataStreamSource<String> stream = env.addSource(FlinkSourceUtil.getKafkaSource(groupId, t));
            topicAndStreamMap.put(t, stream);
```

### 0 .5 FlinkSinkUtil

```java
public class FlinkSinkUtil {
    // 得到一个kafka source
    public static FlinkKafkaProducer<String> getKafkaSink(String topic) {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                topic,
                (KafkaSerializationSchema<String>) (element, timestamp) -> new ProducerRecord<>(topic, null, element.getBytes(StandardCharsets.UTF_8)),
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                    element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
        );
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
    }
```

### 0 .6 JDBCUtil

封装getJdbcConnection方法，传入driver类和url

queryList方法可以执行各种sql，返回值是泛型T的list

```java
 public static Connection getJdbcConnection(String driver,
                                               String url) throws ClassNotFoundException, SQLException {
        Class.forName(driver);
        return DriverManager.getConnection(url);
    }

    public static void main(String[] args) throws Exception {
        Connection conn = getJdbcConnection(Constant.MYSQL_DRIVER, Constant.MYSQL_URL);
        List<JSONObject> list = queryList(conn, "select * from user_info where id='1'", null, JSONObject.class);
        for (JSONObject obj : list) {

            System.out.println(obj);
        }
    }

    // 执行指定sql语句, 并把查询到的结果封装到List集合中
    public static <T> List<T> queryList(Connection conn,
                                        String sql,
                                        Object[] args,
                                        Class<T> tClass) throws Exception {
        PreparedStatement ps = conn.prepareStatement(sql);
        // 1. 先把sql中的占位符进行赋值  args 的长度是几就表示sql中有几个占位符
        for (int i = 0; args != null && i < args.length; i++) {
            ps.setObject(i + 1, args[i]);
        }

        ArrayList<T> result = new ArrayList<>();
        // 2. 执行sql语句
        ResultSet resultSet = ps.executeQuery();
        // 通过 resultSet 获取相关的元数据类得到
        ResultSetMetaData metaData = resultSet.getMetaData();
        while (resultSet.next()) {
            // 3. 遍历到每行数据, 把这些数据封装到 T 类型的对象中
            T t = tClass.newInstance();  // 3.1 利用反射的方式, 创建 t类型的对象
            // 知道属性名和属性只  setAge(10)
            // 遍历每一列
            for (int i = 0; i < metaData.getColumnCount(); i++) {
                String columnName = metaData.getColumnLabel(i + 1);// 列的索引是从1开始
                Object value = resultSet.getObject(columnName);
                BeanUtils.setProperty(t, columnName, value);
            }
            result.add(t);
        }

        return result;
    }
```

### 0 .7 DimUtil

readDimFromPhoenix方法是先拼一个sql语句，之后获取标准的JDBC连接获取数据

​	Phoenix中的数据都是字符串

​	sql中的占位符用数组来存

```java
// 从Phoenix读取维度数据
    public static JSONObject readDimFromPhoenix(Connection conn,
                                                String table,
                                                Object id) throws Exception {
        String sql = "select * from " + table + " where id=?";

        // 可以执行各种sql, 查询的结果应该会有多行
        List<JSONObject> list = JDBCUtil.queryList(conn, sql, new Object[]{id.toString()}, JSONObject.class);

        if (list != null && list.size() > 0) {
            return list.get(0);
        }
        return new JSONObject();

    }

    // 动缓存读取维度数据
    public static JSONObject readDimFromCache(Jedis client,
                                              String table,
                                              Object id) {

        String key = table + ":" + id;
        String jsonStr = client.get(key);

        JSONObject dim = null;
        if (jsonStr != null) {
            dim = JSON.parseObject(jsonStr);
            // 每读一个维度, 应该重新计算一个24小时的过期时间   更新过期时间
            client.expire(key, Constant.DIM_EXPIRE_SECOND);
        }
        return dim;
    }

    // 把维度数据存入缓存中
    private static void saveDimToCache(Jedis client,
                                       String table,
                                       Object id,
                                       JSONObject dim) {

        String key = table + ":" + id;
        String value = dim.toJSONString();

        client.setex(key, Constant.DIM_EXPIRE_SECOND, value);  // 把维度写入到redis中
    }

    public static JSONObject readDim(Connection conn,
                                     Jedis client,
                                     String table,
                                     Object id) throws Exception {
        client.select(1); // 把数据保存1号库

        // 首先去缓存中读取, 缓存没有在从hbase中读取
        JSONObject dim = readDimFromCache(client, table, id);
        if (dim == null) {  // 缓存中没有数据
            System.out.println("从 Phoenix  读取维度:" + table + "  " + id);
            dim = readDimFromPhoenix(conn, table, id);
            // 把这次读到的dim数据存储到缓存中. 否则缓存中永远没有数据
            saveDimToCache(client, table, id, dim);
        } else {
            System.out.println("从 缓存  读取维度: " + table + "  " + id);
        }

        return dim;
    }

    public static void main(String[] args) throws Exception {
        Connection conn = JDBCUtil.getJdbcConnection(Constant.PHOENIX_DRIVER, Constant.PHOENIX_URL);
        JSONObject object = readDimFromPhoenix(conn, "dim_user_info", 50000000);
        System.out.println(object);
    }
```

### 0.8 RedisUtil

获取Redis连接池

#### 注意

如果使用的是连接池，不是关闭客户端，而是将客户端归还给连接池

创建连接池

​	new JedisPool 

​	需要相关配置 new JedisPoolConfig()

​	配置最多连接数setMaxTotal(1000)

​		最多空闲setMaxIdle(100)

​		最少空闲setMinIdle(10)

​		最大等待时间setMaxWaitMillis

​		获取对象时做测试，保证客户端可用，避免串流的情况setTestOnBorrow

​		setTestOnReturn

​		setTestOnCreate

```java
public class RedisUtil {
    private static JedisPool pool;
    public static Jedis getRedisClient() {
        if (pool == null) {
            synchronized (RedisUtil.class) {
                if (pool == null) {
                    JedisPoolConfig conf = new JedisPoolConfig();
                    conf.setMaxTotal(1000);
                    conf.setMaxIdle(100);
                    conf.setMinIdle(10);
                    conf.setMaxWaitMillis(1000 * 60); // 从连接池获取对象的时候, 最多等待的时间
                    conf.setTestOnBorrow(true);
                    conf.setTestOnReturn(true);
                    conf.setTestOnCreate(true);
                    pool = new JedisPool(conf, "hadoop107", 6379);
        return pool.getResource();
```

### 0.9 ThreadPoolUtil

直接创建ThreadPoolExecutor对象，传入参数

LinkedBlockingDeque中存储阻塞队列中最多存储的请求数

```java
        return new ThreadPoolExecutor(
                100,  // 线程池中允许同时运行的最大线程数
                300,  // 线程池中最大的线程数量
                300, // 最大空闲时间
                TimeUnit.SECONDS,
                new LinkedBlockingDeque<>(100)  // 阻塞队列中最多存储的请求数
```

## 函数

### KeyWordUdtf

FlinkSql自定义函数

​	需要继承 系统类TableFunction<Row>，传入的类型是row类型，可以自定义

​	按规则写eval方法，传入 参数是关键词字符串

​	需要依赖工具类	Ikutil， 支持传统词

​	调用IkUtil.analyzer，返回值放入一个集合

​	写出方式为collect(Row.of(w))

```java
@FunctionHint(output = @DataTypeHint("row<word string>"))
public class KeyWordUdtf extends TableFunction<Row> {
    public void eval(String kw){
        Collection<String> kws = IkUtil.analyzer(kw);
        for (String w : kws) {
            collect(Row.of(w));
```



# 1 DWD层: 用户 行为日志

## 1.1 DWDLogApp具体实现

首先要继承BaseAppV1，重写run方法。由于BaseAppV1中init方法不是静态的，所以要想在main方法中调用init，需要先创建DWDLogApp对象。

在创建对象时传入不同的参数

### 主要任务

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

### 实现思路

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

​	1.考虑数据乱序，因此采用eventTime

​	2.添加窗口

​		每个用户生命周期中的第一个窗口中才会有可能设置为新用户

​		按照eventTime排序，最小的应该是新用户记录，其他就是旧用户

​	3.如何确定第一个窗口

​		使用状态记录



ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

​	主流

```java
// 把启动日志写入到主流中
                            out.collect(input);
```

​	侧流输出

```java
// 1. 如果是页面
                            JSONObject page = input.getJSONObject("page");
                            if (page != null) {
                                ctx.output(pageTag, input);
                            }
                            // 2. 如果曝光
                            JSONArray displays = input.getJSONArray("displays");
                            if (displays != null) {
                                for (int i = 0; i < displays.size(); i++) {
                                    JSONObject display = displays.getJSONObject(i);
                                    // 把一些其他信息插入到display中
                                    display.put("ts", input.getLong("ts"));
                                    display.put("page_id", input.getJSONObject("page").getString("page_id"));

                                    display.putAll(input.getJSONObject("common"));

                                    ctx.output(displayTag, display);
```

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

# 2 DWD层: 业务数据

业务数据的来源是由MaxWell从mysql采集得到

但是MaxWell把全部数据统一写入一个Topic, 包括业务数据，也包含维度数据，不利于日后的数据处理

所以DWD业务数据层从Kafka的业务数据ODS层读取数据，经过处理后，将**维度数据保存到Hbase**，将**事实数据写回Kafka**作为业务数据的**DWD层**。

## 2.1 主要任务

接受kafka数据并且过滤空值

实现动态分流

​	实现策略是将动态配置方案存入mysql中

​	视图

https://www.processon.com/diagraming/60dcbdc5e0b34d238be0a17e

## 2.2 具体实现

### 设计动态配置表

将配置表封装成java实体类

```java
public class TableProcess {
    //动态分流Sink常量
    public static final String SINK_TYPE_HBASE = "hbase";
    public static final String SINK_TYPE_KAFKA = "kafka";
    public static final String SINK_TYPE_CK = "clickhouse";
    //来源表
    private String sourceTable;
    //操作类型 insert,update,delete
    private String operateType;
    //输出类型 hbase kafka
    private String sinkType;
    //输出表(主题)
    private String sinkTable;
    ---略
```

### Flink从kafka读取数据

### Flink从mysql读取配置（CDC）

#### ①Flink SQL CDC

​	CDC 全称是 **Change Data Capture** ，捕获变更数据

#### ②Flink SQL内置debeizum

​	业界主要有基于查询的 CDC 和基于日志的 CDC 

#### ③查询CDC和日志CDC对比（重点）

|                          | 查询CDC                                      | 日志CDC                                |
| ------------------------ | :------------------------------------------- | -------------------------------------- |
| 概念                     | 每次捕获变更都发起查询，全表扫表过滤变更数据 | 读取数据库log保持监控 例如mysql binlog |
| 开源产品                 | sqoop kafka jdbc source                      | canal maxwell debezium                 |
| 执行模式                 | batch                                        | streaming                              |
| 捕获所有变化             | 不可以                                       | 可                                     |
| 低延迟，负载小           | 不可以                                       | 可                                     |
| 不侵入业务（lastupdate） | 不可以                                       | 可                                     |
| 捕获删除和旧的           | 不可以                                       | 可                                     |
| 捕获旧的状态             | 不可以                                       | 可                                     |



Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取**全量数据**和**增量变更数据**的 source 组件

<https://github.com/ververica/flink-cdc-connectors>

#### ④代码构建

①读取配置表的数据（flink-sql）

​		获取tableStream运行环境

```java
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
```

​		封装一个方法读取配置表的数据

​		创建一个临时表来存配置表的数据

```java
tEnv.executeSql("CREATE TABLE `table_process` (\n" +
				略字段名
                "  PRIMARY KEY (`source_table`,`operate_type`) not enforced" +
                ")with(" +
                " 'connector' = 'mysql-cdc',\n" +
				略配置信息
                ")");
```

​		将查询到的数据临时表转换为流，流的数据类型用封装的样例类

​		对流数据进行过滤

```java
return tEnv
                .toRetractStream(tpTable, TableProcess.class)
                .filter(t -> t.f0)
                .map(t -> t.f1);
```

​		

②对sourceStream数据做一些ETL

​		map ------> filter

```java
                .map(JSON::parseObject)
                .filter(obj ->
                        obj.getString("database") != null
                                && obj.getString("table") != null
                                && obj.getString("type") != null
                                && (obj.getString("type").contains("insert") || obj.get("type").equals("update"))
                                && obj.getString("data") != null
                                && obj.getString("data").length() >2
```



#### ⑤注意事项

##### ①读取mysql模式的问题

​	snapshot.mode

​		用来控制debezium如何从mysql读数据

​		参数 initial 是程序一启动则首先把表中所有数据读出（通过mysql查询）

​		基于binlog来监控新增和变化

​		参数 never 只是监控变化，不监控原始数据

​	指定格式

```
debezium.snapshot.mode =  initial or never 
```



##### ②Maxwell bootstrap

旧的数据需要通过bootstrap的方式读取，导致obj.getString("type")为bootstrap insert 还会包含bootstrap complete，因此在过滤时要考虑这个问题

### 将配置做成广播流与业务数据进行connect，动态控制业务数据的sink方向

#### 	①配置流做成广播流

​		将查询到的配置数据临时表转换为流对象之后，调用流对象的broadcast方法，其中要传入一个MapStateDescriptor<Object, Object>，也就是说广播流中的数据类型是map，key和value的类型需要决定，依据表名和操作类型来确定sink

```java
MapStateDescriptor<String, TableProcess> tpStateDesc = new MapStateDescriptor<>("tpState",
               String.class,
                TableProcess.class);
                
        BroadcastStream<TableProcess> tpBCstream = tpStream.broadcast(tpStateDesc);
```

#### 	②connect两个流

​	调用数据流的connect方法连接两个流

​	然后调用process方法对数据流的数据和广播流的数据进行处理

```java
return etlStream.connect(tpBCstream)
                .process(new BroadcastProcessFunction<JSONObject,
                        TableProcess, Tuple2<JSONObject,TableProcess>>() {
                    @Override
                    public void processElement(JSONObject jsonObject,
                                               ReadOnlyContext readOnlyContext,
                                               Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        ReadOnlyBroadcastState<String, TableProcess> tpState = readOnlyContext.getBroadcastState(tpStateDesc);
                        String key = jsonObject.getString("table") + ":" + jsonObject.getString("type")
                                .replaceAll("bootstrap-", "");

                        TableProcess tableProcess = tpState.get(key);

                        if (tableProcess != null) {
                            collector.collect(Tuple2.of(jsonObject,tableProcess));
                        }
                    }

                    @Override
                    public void processBroadcastElement(TableProcess tableProcess,
                                                        Context context,
                                                        Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        //获取广播状态
                        BroadcastState<String, TableProcess> TPstate = context.getBroadcastState(tpStateDesc);
                        //把配置写入广播状态
                        String key = tableProcess.getSourceTable() + ":" + tableProcess.getOperateType();
                        TPstate.put(key,tableProcess);
                    }
                });
```

​	其中广播流时把表名和类型写入广播流，数据流是读取广播状态中的数据，并和数据以Tuple2的形式写出

#### 	③数据写出到不同的流

​		封装了dynamicSplitStream方法，传入的参数是SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> dataTpStream也就是添加了广播状态之后的数据流

```java
                .process(new ProcessFunction<Tuple2<JSONObject, TableProcess>, Tuple2<JSONObject, TableProcess>>() {
                    @Override
                    public void processElement(Tuple2<JSONObject, TableProcess> value,
                                               Context ctx,
                                               Collector<Tuple2<JSONObject, TableProcess>> out) throws Exception {

                        // 去kafka放入主流  去hbase的放入侧输出流
                        TableProcess tp = value.f1;
                        //根据配置表中sinkcolum的值对数据进行一些过滤

                        // 1. 根据配置表中sink_columns的值, 数据做一些过滤   100  50需要sink, 把不需要sink的列删除
                        JSONObject data = value.f0.getJSONObject("data");
                        filterColumns(data, tp);

                        // 写入到kafka或者hbase的数据应该只有data中的数据, 不应包含其他的一些元数据了
                        Tuple2<JSONObject, TableProcess> result = Tuple2.of(data, tp);
                        if (TableProcess.SINK_TYPE_KAFKA.equals(tp.getSinkType())) {
                            //                        out.collect(value);
                            out.collect(result);
                        } else if (TableProcess.SINK_TYPE_HBASE.equals(tp.getSinkType())) {
                            // 把到hbase的数据写入到测输出流
                            ctx.output(new OutputTag<Tuple2<JSONObject, TableProcess>>("hbase") {}, result);
```

​		在sink之前数据要根据在配置表中sinkColumn列中指定的字段对数据进行过滤，排除掉不需要的字段，封装了filterColumns(JSONObject data, TableProcess tp)方法

```java
private void filterColumns(JSONObject data, TableProcess tp) {
                        // id,user_id,sku_id,cart_price,sku_num,img_url,sku_name,is_checked,create_time,operate_time,is_ordered,order_time,source_type,source_id
                   /* Set<String> keys = data.keySet();
                    Iterator<String> it = keys.iterator();
                    while (it.hasNext()) {
                        // 如果需要删应该在这个地方删
                    }*/
                        List<String> cs = Arrays.asList(tp.getSinkColumns().split(","));
                        data.keySet().removeIf(key -> !cs.contains(key));
```

#### ④不同流的数据写出到不同的sink		

##### **①Kafka**

调用流的addSink方法，要传入对应的sinkFunction，这里采用自定义的方法

返回值的类型需要是FlinkKafkaProducer的有参构造方法，在其中传入topic，KafkaSerializationSchema，kafka Properties and Semantic

由于写出的数据要进入不同的topic，所以要重载FlinkSinkUtil中的getKafkaSink方法

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                     element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
```

##### **②Hbase**

使用Phoenix写入

​	Phoenix中的表如何创建

​		动态建表

​		建表时机

​			第一条数据来时去建对应维度表

​	向Phoenix写入数据

​		动态写入

​		sql语句根据不同表拼出不同sql

###### 具体实现

对应流调用addSink方法，传入的是自定义工具类FlinkSinkUtil的getHbaseSink方法

可以进行分组，使用keyby，按照SinkTable名进行keyby，提高写入效率

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
```

封装的类PhoenixSink()

自定义sink：继承类RichSinkFunction

```java
public class PhoenixSink extends RichSinkFunction<Tuple2<JSONObject, TableProcess>> {
    private Connection conn;
    private ValueState<Boolean> createTableState;
```

重写open方法，建立Phoenix连接。使用标准的jdbc连接

​			加载驱动（常见数据库不用加载，小众数据库需要加载。例如Phoenix）

​			获取连接对象

```java
 @Override
    public void open(Configuration parameters) throws Exception {
        // 0. 建立到Phoenix的连接. 使用标准的jdbc连接就可以了
        // 1. 加载驱动(常见的数据库可以不用加载, java会根据url自动的加载, 有些数据库比较加载, 比如: Phoenix)
        Class.forName(Constant.PHOENIX_DRIVER);
        // 2. 获取连接对象
        conn = DriverManager.getConnection(Constant.PHOENIX_URL);

        createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

**重写invoke方法，是用来实现对数据的操作业务**

​	先检测表是否存在，如果不存在，创建不存在的表

​	建表：执行sql（建表）来实现在Phoenix中建表

​	1.先拼接一个建表语句

​	2.执行sql

​		先获取连接对象

​		连接对象调用prepareStatement方法（得到预处理语句，传入sql）

​		所有字段都使用varchar数据类型

​		对sql占位符进行赋值

​		执行sql，调用execute方法，之后调用connect的commit方法，最后关闭

```java
@Override
    public void invoke(Tuple2<JSONObject, TableProcess> value,
                       Context context) throws Exception {
        // 1. 检测表是否存在, 如果不存在需要先建表
        checkTable(value);
        // 2.  把这条数据写入到Phoenix中
        write2Hbase(value); }
    private void write2Hbase(Tuple2<JSONObject, TableProcess> value) throws SQLException {
        JSONObject data = value.f0;
        TableProcess tp = value.f1;
        // upsert into user(id, name, age) values(?,?,?)

        StringBuilder sql = new StringBuilder();
        String[] cs = tp.getSinkColumns().split(",");
        sql
                .append("upsert into ")
                .append(tp.getSinkTable())
                .append("(")
                .append(tp.getSinkColumns())
                .append(")values(");
        // 添加占位符 TODO
        // sql.append(tp.getSinkColumns().replaceAll("[^,]+", "?"));  // 正则替换
        for (String c : cs) {
            sql.append("?,");
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")");
        PreparedStatement ps = conn.prepareStatement(sql.toString());
        // 给占位符赋值
        for (int i = 0; i < cs.length; i++) {
            String v = data.get(cs[i]) == null ? "" : data.get(cs[i]).toString();  // 把所有的值变成string,然后就和Phoenix中的varchar对应了  "null" null
            ps.setString(i + 1, v);
        }
        ps.execute();
        conn.commit();
        ps.close();
    }
    // 检测表, 并创建不存在的表
    private void checkTable(Tuple2<JSONObject, TableProcess> value) throws SQLException, IOException {
        // 执行建表语句,实现在Phoenix中完成建表
        if (createTableState.value() == null) {  // 可以避免每来一条数据都需要去执行一次sql
            TableProcess tp = value.f1;
            // 1. 先拼接一个建表语句  TODO
            // create table if not exists t(name varchar, age varchar, constraint pk primary key(name, age))
            StringBuilder sql = new StringBuilder();
            sql
                    .append("create table if not exists ")
                    .append(tp.getSinkTable())
                    .append("(");
        /*for (String c : tp.getSinkColumns().split(",")) {
            sql.append(c).append(" varchar,");
        }
        sql.deleteCharAt(sql.length() - 1); // 去掉最后一个逗号*/
            sql
                    .append(tp.getSinkColumns().replaceAll(",", " varchar,"))
                    .append(" varchar, constraint pk primary key(");
            // 拼接主键
            sql.append(tp.getSinkPk() == null ? "id" : tp.getSinkPk());
            sql
                    .append("))")
                    .append(tp.getSinkExtend() == null ? "" : tp.getSinkExtend());

            // 2. 执行sql语句
            // 2.1 得到预处理语句
            System.out.println("建表语句: " + sql.toString());

            PreparedStatement ps = conn.prepareStatement(sql.toString());
            // 2.2 给sql中的占位符进行赋值 不需要给占位符赋值, 因为没有问号
            // 2.3 执行sql
            ps.execute();
            conn.commit();
            ps.close();
            createTableState.update(true);}}
```

重写close方法

```java
    @Override
    public void close() throws Exception {
        // 是否资源
        if (conn != null) {
            conn.close();
```

###### 动态建表的优化

添加一个状态来判断是不是某个表的第一条数据，是的话再创建表，不用每次都查询Phoenix是否存在表

```java
createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

##### ③盐表

Phoenix Salted Table是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。

通过在创建表的时候指定SALT_BUCKETS来实现pre-split(预分割)。

###### 实现原理

将散列取余后byte值插入owkey第一个字节，通过定义region的start key 和 end key 将数据分割到不同的region

以此来防止自增序列引入的热点问题，从而达到平衡HBase集群的读写性能的目的。
salted byte的计算方式大致如下

hash(rowkey) % SALT_BUCKETS

SALT_BUCKETS的取值为1到256

默认下salted byte将作为每个region的start key 及 end key，以此分割数据到不同的region，这样能做到具有相同salted byte的数据能够位于同一个region里面

#### 数据写入kafka细节

由于数据量小的情况下写入并不是真正的轮询，而是分时间片写入，一段时间内写入一个分区，有可能会造成数据倾斜，解决方法可以指定一个key为null或者手动指定分区，数据量大时不会有这个问题

#### 动态分流的局限性

##### 配置数据流和数据流的到达时间不匹配问题

解决方法

①先启动配置数据流，再启动数据流

②如果还是没有等到配置流

可能的情况：

​	真的没有配置

​	配置未到

​		把配置先存入集合

​		规定时间后再去单独处理（自定义定时器）

##### CDC如何处理采集数据时读锁问题

maxwell和debezium独有的读取旧数据的功能

​	在读取的时候如果有业务数据在写，会出现数据不一致的情况

​	在读取时别的进程没有权限写，等读完才允许写

解决

①关闭读锁，不建议，一般会开启

# 3 DWM层

DWM层的指标和DWS层挂钩

对数据做轻度聚合，例如过滤，例如指存某个用户当天的第一条数据

## 需求概述

| 统计主题 | 需求指标       | 输出方式   | 计算来源                 | 来源层级 |
| -------- | -------------- | ---------- | ------------------------ | -------- |
| 访客     | pv             | 可视化大屏 | page_log直接可求         | dwd      |
|          | uv             | 可视化大屏 | 需要用page_log过滤去重   | dwm      |
|          | 跳出率         | 可视化大屏 | 需要通过page_log行为判断 | dwm      |
|          | 连续访问页面数 | 可视化大屏 | 需要识别开始访问标识     | dwd      |
|          | 连续访问时长   | 可视化大屏 | 需要识别开始访问标识     | dwd      |
| 商品     | 点击           | 多维分析   | page_log直接可求         | dwd      |
|          | 收藏           | 多维分析   | 收藏表                   | dwd      |
|          | 加入购物车     | 多维分析   | 购物车表                 | dwd      |
|          | 下单           | 可视化大屏 | 订单宽表                 | dwm      |
|          | 支付           | 多维分析   | 支付宽表                 | dwm      |
|          | 退款           | 多维分析   | 退款表                   | dwd      |
|          | 评论           | 多维分析   | 评论表                   | dwd      |
| 地区     | pv             | 多维分析   | page_log直接可求         | dwd      |
|          | uv             | 多维分析   | 需要用page_log过滤去重   | dwm      |
|          | 下单           | 可视化大屏 | 订单宽表                 | dwm      |
| 关键词   | 搜索关键词     | 可视化大屏 | 页面访问日志 直接可求    | dwd      |
|          | 点击商品关键词 | 可视化大屏 | 商品主题下单再次聚合     | dws      |
|          | 下单商品关键词 | 可视化大屏 | 商品主题下单再次聚合     | dws      |

**跳出**

访问单个页面后离开

## 3.1 UV

### 数据来源

①启动日志（只包括移动设备）

数据量可能会偏小

②页面日志

### 思路

利用Flink的状态

单日范围内对用户去重

第二条清零

### 具体实现

从DWD层读取日志数据在流中，从流中拿到数据

```java
sourceStream
                    .map(JSONObject::parseObject)
```

拿到的数据可能是乱序，需要用事件时间，添加水印

调用assignTimestampsAndWatermarks，传入WatermarkStrategy

```java
.assignTimestampsAndWatermarks(WatermarkStrategy
                            .<JSONObject>forBoundedOutOfOrderness(Duration.ofSeconds(3)))
```

调用withTimestampAssigner指定时间戳的JSON字段

```java
.withTimestampAssigner((obj, ts) -> obj.getLong("ts")))
```

根据mid分组

```java
.keyBy((obj -> obj.getJSONObject("common").getString("mid")))
```

分配窗口，传入窗口处理函数ProcessWindowFunction

设定一个状态来判断是否是该mid第一次访问

如果到了第二天要清空状态

用SimpleDateFormat来获取当天日期

由于设定了事件时间，如果乱序数据，时间戳大的数据不会进入第一个窗口，所以不会触发窗口关闭更不会触发窗口函数的计算

只有进入第一个窗口且设定的第一次访问状态为空才能证明是第一条数据

```java
.process(new ProcessWindowFunction<JSONObject, JSONObject, String, TimeWindow>() {
                        private ValueState<Long> firstVisitState;
                        private SimpleDateFormat df;
                        @Override
                        public void open(Configuration parameters) {
                            firstVisitState = getRuntimeContext()
                                    .getState(new ValueStateDescriptor<>("firstVisitState", Long.class));
 df = new SimpleDateFormat("yyyy-MM-dd");}
                        @Override
                        public void process(String key, Context context, Iterable<JSONObject> elements, Collector<JSONObject> outer) throws Exception {
                            //如果到了第二天，则首先清空状态
                            //今天
                            String today = df.format(context.window().getEnd());
                            String last = df.format(firstVisitState.value() == null? 0L :firstVisitState);
                            if (!today.equals(last)){
                                firstVisitState.clear();}
                            if (firstVisitState.value() == null){
                                List<JSONObject> list = CommonUtil.toList(elements);
                                JSONObject first = Collections.min(list, Comparator.comparing(o -> o.getLong("ts")));
                                outer.collect(first);
                                firstVisitState.update(first.getLong("ts"));
```

## 3.2 跳出率

跳出：访问了入口页面之后离开

跳出率的分母是访问了入口的所有记录

### 数据来源

页面日志

### 思路

用CEP来实现

按照mid分组，一个用户的访问记录进入一组

只访问入口（不一定是首页），不访问其他

如何确定入口：没有lastPageId	

增加超时时间，来确保入口之后没有访问其他页面

### 具体实现

获取流

```java
KeyedStream<JSONObject, String> stream = sourceStream
                .map(JSON::parseObject)
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<JSONObject>forBoundedOutOfOrderness(Duration.ofSeconds(3))
                                .withTimestampAssigner((obj, ts) -> obj.getLong("ts")))
                .keyBy(obj -> obj.getJSONObject("common").getString("mid"));
```

定义模式（组合模式）

把多个单个模式组合在一起就是组合模式.  组合模式由一个**初始化模式**(.begin(...))开头

严格连续(严格紧邻)

begin中确定过滤数据条件为没有lastVistPage

next中确定pageId和lastPageId不为空

设定超时时间，测试用5秒

```java
Pattern<JSONObject, JSONObject> pattern = Pattern
                .<JSONObject>begin("entry")
                .where(new SimpleCondition<JSONObject>() {
                    @Override
                    public boolean filter(JSONObject value) throws Exception {
                        String lastPageId = value.getJSONObject("page").getString("last_page_id");
                        return lastPageId == null || lastPageId.length() == 0;}})
                .next("second")
                .where(new SimpleCondition<JSONObject>() {
                    @Override
                    public boolean filter(JSONObject value) throws Exception {
// 如果有入口, 后面又跟着其他的页面,这是正常访问.
                        JSONObject page = value.getJSONObject("page");
                        String pageId = page.getString("page_id");
                        String lastPageId = page.getString("last_page_id");
                    /*return (pageId != null && pageId.length() > 0
                        && lastPageId != null && lastPageId.length() > 0);*/
                        return Strings.isNotEmpty(pageId) && Strings.isNotEmpty(lastPageId);} })
                .within(Time.seconds(5));
```

把模式应用到流上

​	调用CEP.pattern，传入定义的模式和流

```java
PatternStream<JSONObject> ps =  CEP.pattern(stream, pattern);
```

取出匹配到或者超时的数据

调用PatternStream对象的select方法

传入OutputTag 和 

PatternTimeoutFunction

其中重写的两个方法分别对应对超时数据和满足条件的数据做处理，这里将超时数据写出

```java
        SingleOutputStreamOperator<JSONObject> result = ps.select(
                new OutputTag<JSONObject>("jump") {},
                new PatternTimeoutFunction<JSONObject, JSONObject>() {
                    @Override
                    public JSONObject timeout(Map<String, List<JSONObject>> pattern,
                                              long timeoutTimestamp) throws Exception {
                        return pattern.get("entry").get(0);  // 获取超时的数据, 就是我们需要的跳出明细
                    }

                },
                new PatternSelectFunction<JSONObject, JSONObject>() {
                    @Override
                    public JSONObject select(Map<String, List<JSONObject>> pattern) throws Exception {
                        return null;  // 满足模式的正常数据, 对我们来说不需要, 可以直接返回null}});
```

## 3.3 订单宽表

### 需求分析与思路

为了减少后期join，提前join得到一张宽表

①实现流与流的join

②join之后的数据读取维度数据，加缓存，加异步

join关系图

https://www.processon.com/diagraming/60de3a26637689510d68ecfd

### Flink中的两种join

interval join 和 window join

### 数据来源

维度数据来自于DIM层，存在Hbase，变化缓慢，不适合存入流

### 具体实现

#### 实现流与流的join

封装join两个事实表的方法，传入两个流，oderInfo和oderDetail

**对流的基本操作**：解析json字符串，解析成对应的表封装类，事件时间，加水印，分组

```java
.get(TOPIC_DWD_ORDER_INFO)
                .map(json -> JSON.parseObject(json, OrderInfo.class))
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<OrderInfo>forBoundedOutOfOrderness(Duration.ofSeconds(3))
                                .withTimestampAssigner((info, ts) -> info.getCreate_ts())

                )
                .keyBy(OrderInfo::getId);
```

对这两个流进行interval join

其中join时要规定数据的时间段，例如同一个订单的数据到来不会相隔很久的时间

于是设置between(Time.minutes(-5), Time.minutes(5))

之后调用process方法对其进行处理

```java
orderInfoStream
                .intervalJoin(orderDetailStream)
                .between(Time.minutes(-5), Time.minutes(5))
                // join 完成应该是返回一张宽表, 这张宽表目前维度只有一些id , 其实是缺少一些维度信息
                .process(new ProcessJoinFunction<OrderInfo, OrderDetail, OrderWide>() {
                    @Override
                    public void processElement(OrderInfo left,
                                               OrderDetail right,
                                               Context ctx,
                                               Collector<OrderWide> out) throws Exception {
                        out.collect(new OrderWide(left, right));
```

#### join之后的数据读取维度数据，加缓存，加异步

封装join维度信息的方法，传入双流join之后的流

每次来一条数据，通过Phoenix查询一次，join一次

调用双流join的map方法，传入RichMapFunction

​	因为每次来数据join之后数据量不变，因此将数据存入map中

​	在重写的open方法中建立连接Phoenix，连接Phoenix也是使用jdbc，于是封装jdbc类

##### 	在重写的map方法中读取数据

​	封装读取不同表的工具类DimUtil，基本实现思路是在Phoenix中执行sql，用传参的方式拼成查询不同表的sql

​	调用DimUtil的readDimFromPhoenix方法，传参是JDBC连接对象，表名以及字段名

只列出一个表的代码，实际上读取了

DIM_USER_INFO

DIM_BASE_PROVINCE

DIM_SKU_INFO

DIM_BASE_TRADEMARK

DIM_SPU_INFO

DIM_BASE_CATEGORY3 的某些相关字段

```java
                        // 6. 读取c3信息
                        JSONObject c3Info = DimUtil.readDimFromPhoenix(conn, Constant.DIM_BASE_CATEGORY3, orderWide.getCategory3_id());
                        orderWide.setCategory3_name(c3Info.getString("NAME"));
                        return orderWide;
```

#### ①优化一 添加旁路缓存（重点）

每次读DIM数据都连接Phoenix，这样效率不高

把用到的维度信息存储进内存，提升查询速度

​	先从缓存读，缓存有直接使用，缓存没有再去查询数据库

①使用Flink的map状态存

​	好处：本地缓存，无需访问网络

​	坏处：如果数据量大，内存压力大

​	           如果维度数据发生变化，缓存无法更新

​		   CDC也不可以解决，只支持msql

②使用外置缓存（使用Redis）

​	专门的缓存数据库，任何应用都可以访问

​	DWD层在分流时更新第三方缓存中的数据，DWM层也可以访问到第三方缓存的数据

##### 具体实现

封装带缓存读取DIM数据的方法 readDim，传入连接对象，Redis客户端，表明，id

调用redisK客户端的select方法选择存储的库

```java
public static JSONObject readDim(Connection conn,
                                     Jedis client,
                                     String table,
                                     Object id) throws Exception {
        client.select(1); // 把数据保存1号库

        // 首先去缓存中读取, 缓存没有在从hbase中读取
        JSONObject dim = readDimFromCache(client, table, id);
        if (dim == null) {  // 缓存中没有数据
            System.out.println("从 Phoenix  读取维度:" + table + "  " + id);
            dim = readDimFromPhoenix(conn, table, id);
            // 把这次读到的dim数据存储到缓存中. 否则缓存中永远没有数据
            saveDimToCache(client, table, id, dim);
        } else {
            System.out.println("从 缓存  读取维度: " + table + "  " + id);
        }
```

首先去缓存中读取，缓存没有去Phoenix读取，所以封装去内存读取数据的方法readDimFromCache，传入的对象时Redis客户端，表名和id

```java
 public static JSONObject readDimFromCache(Jedis client,
                                              String table,
                                              Object id) {
        String key = table + ":" + id;
        String jsonStr = client.get(key);

        JSONObject dim = null;
        if (jsonStr != null) {
            dim = JSON.parseObject(jsonStr);
            // 每读一个维度, 应该重新计算一个24小时的过期时间   更新过期时间
            client.expire(key, Constant.DIM_EXPIRE_SECOND);
        }
        return dim;
```

还要封装从Phoenix读取的方法readDimFromPhoenix

```java
public static JSONObject readDimFromPhoenix(Connection conn,
                                                String table,
                                                Object id) throws Exception {
        String sql = "select * from " + table + " where id=?";

        // 可以执行各种sql, 查询的结果应该会有多行
        List<JSONObject> list = JDBCUtil.queryList(conn, sql, new Object[]{id.toString()}, JSONObject.class);

        if (list != null && list.size() > 0) {
            return list.get(0);
        }
        return new JSONObject();
```

最后需要将从Phoenix读到的数据写到缓存（Redis），仍然封装方法saveDimToCache

传入Redis客户端，连接对象，表名和dim数据

调用redis客户端的setex方法，将数据存入，设置过期时间为一天

```java
private static void saveDimToCache(Jedis client,
                                       String table,
                                       Object id,
                                       JSONObject dim) {
        String key = table + ":" + id;
        String value = dim.toJSONString();

        client.setex(key, Constant.DIM_EXPIRE_SECOND, value);  // 把维度写入到redis中}
```

##### 维度数据存入Redis时的数据结构

String List Set Hash（map）Zset 五大数据结构

考虑读取方便

String：“表名：id”可以查到指定dim信息

缺点：key太多，不方便管理

​	优化：所有的维度存到一个单独的库中，不用默认的0号库

Hash：

| key  | field | value               |
| ---- | ----- | ------------------- |
| 表名 | id    | dim的json格式字符串 |

考虑可以给维度数据设置过期时间

String：每个key可以单独设置过期时间

Hash：无法给每个field单独设置过期时间

##### 在DwdDBapp中实现缓存的更新和删除

时机是数据分流时，对应继承了RichSinkFunction的PhoenixSink类

在重写的invoke方法中调用封装的方法

拼写对应的String key，根据key调用redis的客户端方法del来删除缓存中的数据

```java
    private void delCache(Tuple2<JSONObject, TableProcess> value) {

        String key = value.f1.getSinkTable().toUpperCase() + ":" + value.f0.get("id");

        Jedis jedis = RedisUtil.getRedisClient();
        jedis.select(1);
        jedis.del(key);  // 如果key不存在, 会有啥影响?  没有影响. 所以不用提前判断key是否存在
        jedis.close();  // 归还给连接池
```

#### ②优化二 异步IO（重点）

每次来一条事实数据要去join维度数据时都去查询维度数据，默认是同步方式进行网络连接

Redis和Phoenix都需要连接，网络连接和传输很耗时

flink1.2之后支持异步的api

​	默认模式下flink在mapFunction中只采用同步的方式去交互

异步化中，单个并行度可以连续发出多个请求，哪个先返回就先处理哪个，从而在连续的请求期间不需要阻塞形式等待，大大提高了流处理效率

##### 使用异步IO的条件

支持异步请求的数据库客户端，Phoenix和Redis现在还没有异步的客户端，需要自定义线程池的方式来防止阻塞，将请求放入线程池，但是效率不及正规的异步客户端

##### 异步IO框架的搭建

在事实表和维度表join制作宽表时使用异步的方式

修改封装的dimJoin方法

使用Flink提供的api AsyncDataStream

​	调用unorderedWait方法，传入数据流，AsyncFunction，超时时间（请求超时时间），时间单位

​	封装异步的 处理函数DimAsyncFunction(实现RichAsyncFunction)

​	在重写的asyncInvoke方法中

​		Phoenix不支持异步方式读，需要使用线程来完成异步请求

​		asyncInvoke传入的是input和ResultFuture<T> resultFuture，input是数据流，在这个场景下是没赢维度数据的事实数据，在asyncInvoke方法中读到维度数据，把事实数据对应的维度数据补齐之后交还给 resultFuture放入之后的流中

​	封装一个Util类来获取线程池ThreadPoolUtil

​	调用线程池的execute方法，在重写的run方法中实现业务：查询Dim数据，补齐维度使用addDim方法

​		

```java
    private ThreadPoolExecutor threadPool;
    private Connection conn;

    public abstract void addDim(Connection conn,
                                Jedis jedis,
                                T input,
                                ResultFuture<T> resultFuture) throws Exception;
    @Override
    public void open(Configuration parameters) throws Exception {
        threadPool = ThreadPoolUtil.getThreadPool();
        conn = JDBCUtil.getJdbcConnection(Constant.PHOENIX_DRIVER, Constant.PHOENIX_URL); }
    @Override
    public void close() throws Exception {
        if (threadPool != null) {
            threadPool.shutdown();
        }
        if (conn != null) {
            conn.close();
        } }
    // 异步调用, 每碰到一条数据, Flink会自动的调用这个方法, 可以把我们的业务处理逻辑放在这里
    @Override
    public void asyncInvoke(T input,  // 输入的数据
                            ResultFuture<T> resultFuture) throws Exception {
        // Phoenix客户端目前不支持异步方式进行读, 所以我们需要自己使用线程来完成这个异步的请求
        // 把input的补齐维度之后, 就可以把他交给 resultFuture, 这个数据就会进入流的下个阶段
        threadPool.execute(new Runnable() {
            @Override
            public void run() {
                // dim查询工作
                // redis客户端使用的连接池, 应该一个异步请求使用一个客户端, 否则会出现问题
                Jedis jedis = RedisUtil.getRedisClient();
                // 具体的维度补充的业务, 这个业务是比较抽象的, 和使用者有关系
                try {
                    addDim(conn, jedis, input, resultFuture);
                } catch (Exception e) {
                    throw new RuntimeException("异步读取维度数据出错....");
                }jedis.close(); }
```

## 3.4 支付宽表

### 需求概述

订单的支付情况，每个商品的支付情况，例如每个商品被支付过多少次

支付表的信息和订单明细进行关联

### 思路

由于支付表信息一般在对应订单详情数据到来之后45分钟之后才到来，因此join方式只能使用inervalJoin，将下界设置在45分钟以上

另一种思路是每次到来支付信息表的数据，去数据库查对应的订单详情表数据，这样订单详情表数据不用存入缓存，但是网络效率低

第三种是将实现的订单宽表的信息和支付表以流的方式进行join，省去了查询维度表的步骤，仍然需要将上下界设置的大一些

### 具体实现

采用第三种思路

创建了继承BaseAppv2的类，在重写的run方法中先获取订单宽表和支付表作为流

分别进行flink事件基本操作，map取指定数据，assignTimestampsAndWatermarks添加水印，keyBy进行分组，只列出TOPIC_DWD_PAYMENT_INFO的代码

```java
KeyedStream<PaymentInfo, Long> paymentInfoStream = dsMap
                .get(TOPIC_DWD_PAYMENT_INFO)
                .map(json -> JSON.parseObject(json, PaymentInfo.class))
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<PaymentInfo>forBoundedOutOfOrderness(Duration.ofSeconds(3))
                                .withTimestampAssigner((pay, ts) -> CommonUtil.toTs(pay.getCreate_time()))
                )
                .keyBy(PaymentInfo::getOrder_id);
```

对两个流进行interVal Join，调用intervalJoin方法，下界设置为45分钟

之后将数据写入kafka对应topic DWM_PAYMENT_WIDE

```java
paymentInfoStream
                .intervalJoin(orderWideStream)
                .between(Time.minutes(-45), Time.seconds(5))  // 由于支付要远远的晚于订单宽表
                .process(new ProcessJoinFunction<PaymentInfo, OrderWide, PaymentWide>() {
                    @Override
                    public void processElement(PaymentInfo left,
                                               OrderWide right,
                                               Context ctx,
                                               Collector<PaymentWide> out) throws Exception {
                        out.collect(new PaymentWide(left, right));
                    }
                })
                .map(JSON::toJSONString)
                .addSink(FlinkSinkUtil.getKafkaSink(Constant.TOPIC_DWM_PAYMENT_WIDE));
```

# 4 DWS层

**数据流向图更新**

https://www.processon.com/diagraming/60dad6caf346fb5e35bb1ec1

**数据库的选择**

**要求**

​	支持sql 排除kafka

​	支持大数据量 排除mysql

​	考虑查询速度 Hbase + Phoenix 可选

​	首选ClickHouse

**数据来源**

为DWS层服务的DWM层，DWM层只对数据做了一些加工，提取出了一些指标例如UV，跳出率等

设计一张DWS层的表其实就两件事：维度和度量(事实数据) 

度量包括PV、UV、跳出次数、连续访问页面数、连续访问时长

维度包括在分析中比较重要的几个字段：渠道、地区、版本、新老用户进行聚合

**主题：**

访客，商品，地区，搜索关键词，商品行为关键词

**定位**：

轻度聚合，小窗口级别的聚合，因为如果把所有的聚合都交给ADS层的话ADS层压力比较大

## 4.1 访客主题宽表

## 指标

| 访客           | pv         | 可视化大屏                         | page_log直接可求 |
| -------------- | ---------- | ---------------------------------- | ---------------- |
| uv             | 可视化大屏 | 需要用page_log过滤去重             | dwm              |
| 跳出率         | 可视化大屏 | 需要通过跳出明细和page_log行为判断 | dwd/dwm          |
| 连续访问页面数 | 可视化大屏 | 需要识别开始访问标识               | dwd              |
| 连续访问时长   | 可视化大屏 | 需要识别开始访问标识               | dwd              |

设计一张DWS层的表其实就两件事：维度和度量(事实数据) 

## 具体实现

每个指标的数据源不同，每个指标作为一个窗口的不同列

实现方法：

​	多表join

​	connect

​	union（要求类型一致）

例：

| 聚合窗口 | pv   | uv   | uj   |
| -------- | ---- | ---- | ---- |
| 0-5      | 100  | 0    | 0    |
| 0-5      | 0    | 20   | 0    |
| 0-5      | 0    | 0    | 30   |
| 0-5      | 100  | 20   | 30   |

没有数据的字段值为0

创建类DwsVisitorStatsApp，由于要从不同源读数据，继承BaseAppV2

**消费的topic：**

​	TOPIC_DWD_PAGE_LOG

​	TOPIC_DWM_UV

​	TOPIC_DWM_USER_JUMP_DETAIL

其中连续访问时长和连续访问页面也来自于DWD_PAGE_LOG

拿到数据之后：

①解析流中的数据为同一类型，然后union，封装了parseAndUnion方法，传入的是存在map中的流

​	拿到的流数据封装到样例类VisitorStats

```java
public class VisitorStats {
    //统计开始时间
    private String stt;
    //统计结束时间
    private String edt;
    //维度：版本
    private String vc;
    //维度：渠道
    ... //维度：地区//维度：新老用户标识 //度量：独立访客数//度量：页面访问数//度量： 进入次数//度量： 跳出次数//度量： 持续访问时间//统计时间}
```

​	计算 pv  和 持续访问时间：从pageStream中取数据

​		持续访问时间就是每个页面的访问时间之和

​		基础操作map取数据，转换成json对象

​		一进一出，选map或者process，返回值封装进样例类VisitorStats，根据页面日志格式取到指定value

```java
SingleOutputStreamOperator<VisitorStats> pvAndDuringTimeStatsStream = pageStream
                .map(s -> {
                    JSONObject obj = JSON.parseObject(s);
                    JSONObject common = obj.getJSONObject("common");
                    JSONObject page = obj.getJSONObject("page");

                    return new VisitorStats(
                            "", "",
                            common.getString("vc"),
                            common.getString("ch"),
                            common.getString("ar"),
                            common.getString("is_new"),
                            0L, 1L, 0L, 0L, page.getLong("during_time"),
                            obj.getLong("ts")
```

​	制作水印

```java
.assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<VisitorStats>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                                .withTimestampAssigner((vs, ts) -> vs.getTs())
```

​		计算uv：从DWM_UV对应流中取数据

​		操作同上

```java
SingleOutputStreamOperator<VisitorStats> uvStatsStream = uvStream
                .map(s -> {
                    JSONObject obj = JSON.parseObject(s);
                    JSONObject common = obj.getJSONObject("common");

                    return new VisitorStats(
                            "", "",
                            common.getString("vc"),
                            common.getString("ch"),
                            common.getString("ar"),
                            common.getString("is_new"),
                            1L, 0L, 0L, 0L, 0L,
                            obj.getLong("ts")
                    );
                })
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<VisitorStats>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                                .withTimestampAssigner((vs, ts) -> vs.getTs())
```

​		获取跳出次数

只是在封装的VisitorStats中的对应属性值为1

```java
    //度量： 跳出次数
    private Long uj_ct = 0L;
```

​		进入次数的计算：lastVistPage为空

​		这时有些数据计算，有些不计算，所以用process或者flatmap

​		过程是基础处理，判断if (Strings.isEmpty(lastPageId))来过滤

​		满足的话将

```java
//度量： 进入次数
    private Long sv_ct = 0L;
```

​		值为1

代码：

```java
SingleOutputStreamOperator<VisitorStats> svStatsStream = pageStream
                .flatMap(new FlatMapFunction<String, VisitorStats>() {
                    @Override
                    public void flatMap(String s,
                                        Collector<VisitorStats> out) throws Exception {
                        JSONObject obj = JSON.parseObject(s);
                        JSONObject common = obj.getJSONObject("common");
                        JSONObject page = obj.getJSONObject("page");

                        String lastPageId = page.getString("last_page_id");
                        if (Strings.isEmpty(lastPageId)) {
                            VisitorStats vs = new VisitorStats(
                                    "", "",
                                    common.getString("vc"),
                                    common.getString("ch"),
                                    common.getString("ar"),
                                    common.getString("is_new"),
                                    0L, 0L, 1L, 0L, 0L,
                                    obj.getLong("ts")
                            );
                            out.collect(vs);
```

把以上得到的四个流union到一起

​	调用page流的union方法

```java
return pvAndDuringTimeStatsStream
.union(uvStatsStream, 
       ujStatsStream, 
       svStatsStream);							
```

②分组开窗聚合

封装方法aggregateAndWindow，传入刚才union后的流进行分组和聚合

​	首先流数据基本操作，添加水印

​	之后keyby分组

​	制作窗口

​	使用窗口处理函数进行聚合，reduce，aggregate，process 其中process一般用于topN

​	重写reduce方法，聚合tPv_ct，Uv_ct，Sv_ct，Uj_ct，Dur_sum

​	添加窗口函数new ProcessWindowFunction

​	获取窗口开始结束时间，调用上下文的window函数

​	迟到数据采用测流输出

```java
/*.assignTimestampsAndWatermarks(
                    WatermarkStrategy
                        .<VisitorStats>forBoundedOutOfOrderness(Duration.ofSeconds(20))
                        .withTimestampAssigner((vs, ts) -> vs.getTs())
                )*/
                .keyBy(vs -> vs.getVc() + "_" + vs.getCh() + "_" + vs.getAr() + "_" + vs.getIs_new())
                .window(TumblingEventTimeWindows.of(Time.seconds(5)))
                .sideOutputLateData(new OutputTag<VisitorStats>("late") {})
                .reduce(
                        new ReduceFunction<VisitorStats>() {
                            @Override
                            public VisitorStats reduce(VisitorStats vs1, VisitorStats vs2) throws Exception {

                                vs1.setPv_ct(vs1.getPv_ct() + vs2.getPv_ct());
                                vs1.setUv_ct(vs1.getUv_ct() + vs2.getUv_ct());
                                vs1.setSv_ct(vs1.getSv_ct() + vs2.getSv_ct());
                                vs1.setUj_ct(vs1.getUj_ct() + vs2.getUj_ct());
                                vs1.setDur_sum(vs1.getDur_sum() + vs2.getDur_sum());
                                return vs1;
                            }
                        },
                        new ProcessWindowFunction<VisitorStats, VisitorStats, String, TimeWindow>() {
                            @Override
                            public void process(String key,
                                                Context ctx,
                                                Iterable<VisitorStats> elements,  // 一定只有一个值
                                                Collector<VisitorStats> out) throws Exception {
                                VisitorStats vs = elements.iterator().next();
                                TimeWindow w = ctx.window();
                                SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
                                vs.setStt(sdf.format(w.getStart()));
                                vs.setEdt(sdf.format(w.getEnd()));

                                out.collect(vs);
```

**调优**：

将水印添加位置提前，在流union之前就添加水印

③数据写入到ClickHouse

封装sendToClickhouse方法，调用聚合流的addSink方法，传入FlinkSinkUtil.getClickhouseSink方法

jdbc.sink需要的参数：sql，jdbcStatementBuilder（为sql中的占位符进行赋值）

自定义封装getClickhouseSink方法

​	拼接向clickHouse写数据的sql

```java
 sql.append("insert into ")
                .append(table)
                .append("(");
        // TODO 拼接字段名
        Field[] fields = tClass.getDeclaredFields();
        for (Field field : fields) {
            // 获取nosink注解, 如果为空, 则表示数据需要sink到clickhouse中
            NoSink noSink = field.getAnnotation(NoSink.class);
            if (noSink == null) {
                sql.append(field.getName()).append(",");
            }
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")values(");

        // TODO 根据前面字段的个数, 拼接 ?
        for (Field field : fields) {
            NoSink noSink = field.getAnnotation(NoSink.class);
            if (noSink == null) {
                sql.append("?,");
```



```java
    public static <T> SinkFunction<T> getClickhouseSink(String db,
                                                        String table,
                                                        Class<T> tClass) {
        String driver = Constant.CLICKHOUSE_DRIVER;
        String url = Constant.CLICKHOUSE_URL_PRE + db;

        StringBuilder sql = new StringBuilder();
        // 拼接一个向clickhouse写数据的sql
        // insert into t(a, b, c) values(?,?,?)
        sql.append("insert into ")
                .append(table)
                .append("(");
        // TODO 拼接字段名
        Field[] fields = tClass.getDeclaredFields();
        for (Field field : fields) {
            // 获取nosink注解, 如果为空, 则表示数据需要sink到clickhouse中
            NoSink noSink = field.getAnnotation(NoSink.class);
            if (noSink == null) {
                sql.append(field.getName()).append(",");
            }
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")values(");

        // TODO 根据前面字段的个数, 拼接 ?
        for (Field field : fields) {
            NoSink noSink = field.getAnnotation(NoSink.class);
            if (noSink == null) {
                sql.append("?,");
            }
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")");
        return getJdbcSink(driver, url, sql.toString());
```

## 4.2 商品主题宽表



| 统计主题   | 需求指标   | 输出方式                | 计算来源             | 来源层级 |
| ---------- | ---------- | ----------------------- | -------------------- | -------- |
| 商品       | 点击       | 多维分析                | dwd_page_log直接可求 | dwd      |
| 曝光       | 多维分析   | dwd_display_log直接可求 | dwd                  |          |
| 收藏       | 多维分析   | 收藏表                  | dwd                  |          |
| 加入购物车 | 多维分析   | 购物车表                | dwd                  |          |
| 下单       | 可视化大屏 | 订单宽表                | dwm                  |          |
| 支付       | 多维分析   | 支付宽表                | dwm                  |          |
| 退款       | 多维分析   | 退款表                  | dwd                  |          |
| 评价       | 多维分析   | 评价表                  | dwd                  |          |

与访客的dws层的宽表类似，也是把多个事实表的明细数据汇总起来组合成宽表

### 需求分析与思路

1. 从Kafka主题中获得数据流
2. 把Json字符串数据流转换为统一数据对象的数据流
3. 把统一的数据结构流合并为一个流
4. 设定事件时间与水位线
5. 分组、开窗、聚合
6. 写入ClickHouse

### 数据来源

​	点击：pagelog	

​	曝光：displaylog	

​	收藏：收藏表

​	加购：购物车表

​	下单：订单宽表

​	支付：支付宽表

​	退款：退款表

​	评价：评价表

### 具体实现

​	建表

​	封装数据实体类

```java
public class ProductStats {
    
    private String stt;//窗口起始时间
    private String edt;  //窗口结束时间
    private Long sku_id; //sku编号
    private String sku_name;//sku名称
    private BigDecimal sku_price; //sku单价
    private Long spu_id; //spu编号
    private String spu_name;//spu名称
    private Long tm_id; //品牌编号
    private String tm_name;//品牌名称
    private Long category3_id;//品类编号
    private String category3_name;//品类名称
    
    private Long display_ct = 0L; //曝光数
    
    private Long click_ct = 0L;  //点击数
    
    private Long favor_ct = 0L; //收藏数
    
    private Long cart_ct = 0L;  //添加购物车数
    
    private Long order_sku_num = 0L; //下单商品个数
    
    //下单商品金额  不是整个订单的金额
    private BigDecimal order_amount = BigDecimal.ZERO;
    
    private Long order_ct = 0L; //订单数
    
    //支付金额
    private BigDecimal payment_amount = BigDecimal.ZERO;
    
    private Long paid_order_ct = 0L;  //支付订单数
    
    private Long refund_order_ct = 0L; //退款订单数
    
    private BigDecimal refund_amount = BigDecimal.ZERO;
    
    private Long comment_ct = 0L;//评论订单数
    
    private Long good_comment_ct = 0L; //好评订单数
    
    private Set<Long> orderIdSet = new HashSet<>();  //用于统计订单数
    
    private Set<Long> paidOrderIdSet = new HashSet<>(); //用于统计支付订单数
    
    private Set<Long> refundOrderIdSet = new HashSet<>();//用于退款支付订单数
    
    private Long ts; //统计时间戳
```

​	消费8个topic的数据

​	8个topic的数据union到一起

对每个topic来的数据进行基础处理：获取数据流之后调用process方法，根据条件过滤需要的数据

举例下单数据

```java
        SingleOutputStreamOperator<ProductStats> productOrderStatsStream = dsMap
                .get(TOPIC_DWM_ORDER_WIDE)
                .map(jsonStr -> {
                    OrderWide orderWide = JSON.parseObject(jsonStr, OrderWide.class);

                    ProductStats ps = new ProductStats();
                    ps.setSku_id(orderWide.getSku_id());
                    ps.setTs(CommonUtil.toTs(orderWide.getCreate_time()));
                    ps.setOrder_amount(orderWide.getSplit_total_amount());  // 这个商品在这个订单里占的金额
                    ps.setOrder_sku_num(orderWide.getSku_num());

                    // 等最后聚合的时候获取set的长度即可
                    ps.getOrderIdSet().add(orderWide.getOrder_id()); // 这个用来统计这个商品被多个订单下过
                    return ps;});
```

​	开窗聚合

​	封装方法aggregateByWindowAndSkuId，添加水印，keyby分组，window开窗，reduce聚合

```java
.assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<ProductStats>forBoundedOutOfOrderness(Duration.ofSeconds(20))
                                .withTimestampAssigner((ps, ts) -> ps.getTs())
                )
                .keyBy(ProductStats::getSku_id)
                .window(TumblingEventTimeWindows.of(Time.seconds(5)))
                .sideOutputLateData(new OutputTag<ProductStats>("late") {})
                .reduce(
                        new ReduceFunction<ProductStats>() {
                            @Override
                            public ProductStats reduce(ProductStats ps1,
                                                       ProductStats ps2) throws Exception {

                                ps1.setDisplay_ct(ps1.getDisplay_ct() + ps2.getDisplay_ct());
                                ps1.setClick_ct(ps1.getClick_ct() + ps2.getClick_ct());
                                ps1.setFavor_ct(ps1.getFavor_ct() + ps2.getFavor_ct());
                                ps1.setCart_ct(ps1.getCart_ct() + ps2.getCart_ct());

                                ps1.setOrder_amount(ps1.getOrder_amount().add(ps2.getOrder_amount()));
                                ps1.setOrder_sku_num(ps1.getOrder_sku_num() + ps2.getOrder_sku_num());
                                ps1.getOrderIdSet().addAll(ps2.getOrderIdSet());

                                ps1.setPayment_amount(ps1.getPayment_amount().add(ps2.getPayment_amount()));
                                ps1.getPaidOrderIdSet().addAll(ps2.getPaidOrderIdSet());

                                ps1.setRefund_amount(ps1.getRefund_amount().add(ps2.getRefund_amount()));
                                ps1.getRefundOrderIdSet().addAll(ps2.getRefundOrderIdSet());

                                ps1.setComment_ct(ps1.getComment_ct() + ps2.getComment_ct());
                                ps1.setGood_comment_ct(ps1.getGood_comment_ct() + ps2.getGood_comment_ct());

                                return ps1;
                            }
                        },
                        new ProcessWindowFunction<ProductStats, ProductStats, Long, TimeWindow>() {
                            @Override
                            public void process(Long key,
                                                Context context,
                                                Iterable<ProductStats> elements,
                                                Collector<ProductStats> out) throws Exception {
                                TimeWindow w = context.window();
                                ProductStats ps = elements.iterator().next();
                                SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
                                ps.setStt(sdf.format(w.getStart()));
                                ps.setEdt(sdf.format(w.getEnd()));

                                // 和订单相关的3个count进行计算
                                ps.setOrder_ct((long)ps.getOrderIdSet().size());
                                ps.setPaid_order_ct((long)ps.getPaidOrderIdSet().size());
                                ps.setRefund_order_ct((long)ps.getRefundOrderIdSet().size());

                                out.collect(ps);
```

​	补齐数据封装类的维度信息

```java
private SingleOutputStreamOperator<ProductStats> joinDim(SingleOutputStreamOperator<ProductStats> aggregatedStream) {
        return AsyncDataStream.unorderedWait(
                aggregatedStream,
                new DimAsyncFunction<ProductStats>() {
                    @Override
                    public void addDim(Connection conn,
                                       Jedis jedis,
                                       ProductStats input,
                                       ResultFuture<ProductStats> resultFuture) throws Exception {
                        // 1. 补齐sku信息, 和 spu_id, tm_id c3_id
                        JSONObject skuInfo = DimUtil.readDim(conn, jedis, DIM_SKU_INFO, input.getSku_id());
                        input.setSku_name(skuInfo.getString("SKU_NAME"));
                        input.setSku_price(skuInfo.getBigDecimal("PRICE"));

                        input.setSpu_id(skuInfo.getLong("SPU_ID"));
                        input.setTm_id(skuInfo.getLong("TM_ID"));
                        input.setCategory3_id(skuInfo.getLong("CATEGORY3_ID"));

                        // 2. 补齐spu信息
                        JSONObject spuInfo = DimUtil.readDim(conn, jedis, DIM_SPU_INFO, input.getSpu_id());
                        input.setSpu_name(spuInfo.getString("SPU_NAME"));
                        // 3. 补齐tm信息
                        JSONObject tmInfo = DimUtil.readDim(conn, jedis, DIM_BASE_TRADEMARK, input.getTm_id());
                        input.setTm_name(tmInfo.getString("TM_NAME"));
                        // 3. 补齐c3信息
                        JSONObject c3Info = DimUtil.readDim(conn, jedis, DIM_BASE_CATEGORY3, input.getCategory3_id());
                        input.setCategory3_name(c3Info.getString("NAME"));

                        resultFuture.complete(Collections.singletonList(input));
                    }
                },
                300,
                TimeUnit.SECONDS);
```

​	数据发送到clickHouse

```java
    private void sendToClickhouse(SingleOutputStreamOperator<ProductStats> stream) {
        stream.addSink(FlinkSinkUtil.getClickhouseSink("gmall2021", "product_stats_2021", ProductStats.class));}
```

## 4.3 地区主题宽表

### 需求分析及思路

数据来自于oderWider表，已经有相关地区信息

### 考虑用FlinkSql实现

​	导入ClickHouse连接器

​		由于maven仓库没有加入该连接器，需要自己下载之后用maven编译到maven仓库

​	阿里云有实现好的connector, 我们使用这个connector.参考地址: https://help.aliyun.com/document_detail/185696.html?spm=a2c4g.11186623.6.574.d9c541ea3J78mc

实现代码

```java
ublic class DWSProvinceStatsSqlApp {
    public static void main(String[] args) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.enableCheckpointing(5000, CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setCheckpointTimeout(60000);
        env
            .getCheckpointConfig()
            .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
        env.setStateBackend(new FsStateBackend("hdfs://hadoop162:8020/gmall2021/flink/checkpoint2"));
        
        final StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
        // 1. 注册SourceTable: 从Kafka读数据
        tenv.executeSql("CREATE TABLE order_wide (" +
                            "   province_id BIGINT, " +
                            "   province_name STRING," +
                            "   province_area_code STRING," +
                            "   province_iso_code STRING," +
                            "   province_3166_2_code STRING," +
                            "   order_id STRING, " +
                            "   split_total_amount DOUBLE," +
                            "   create_time STRING, " +
                            "   rowtime AS TO_TIMESTAMP(create_time)," +
                            "   WATERMARK FOR  rowtime  AS rowtime - interval '5' second )" +
                            "WITH (" +
                            "   'connector' = 'kafka'," +
                            "   'topic' = 'dwm_order_wide'," +
                            "   'properties.bootstrap.servers' = 'hadoop162:9029,hadoop163:9092,hadoop164:9092'," +
                            "   'properties.group.id' = 'DWSProvinceStatsSqlApp'," +
                            "   'scan.startup.mode' = 'latest-offset'," +
                            "   'format' = 'json'" +
                            ")");
        
        // 2. 注册SinkTable: 向ClickHouse写数据
        tenv.executeSql("create table province_stats_2021(" +
                            "   stt string," +
                            "   edt string," +
                            "   province_id bigint," +
                            "   province_name string," +
                            "   area_code string," +
                            "   iso_code string," +
                            "   iso_3166_2 string," +
                            "   order_amount decimal(20, 2)," +
                            "   order_count bigint, " +
                            "   ts bigint, " +
                            "   PRIMARY KEY (stt, edt, province_id) NOT ENFORCED" +
                            ")with(" +
                            "   'connector' = 'clickhouse', " +
                            "   'url' = 'clickhouse://hadoop162:8123', " +
                            "   'database-name' = 'gmall2021', " +
                            "   'table-name' = 'province_stats_2021'," +
                            "   'sink.batch-size' = '100', " +
                            "   'sink.flush-interval' = '1000', " +
                            "   'sink.max-retries' = '3' " +
                            ")");
        // 3. 从SourceTable查询数据, 并写入到SinkTable
        //        tenv.sqlQuery("select * from order_wide").execute().print();
        tenv.executeSql("insert into province_stats_2021 " +
                            "select " +
                            "   DATE_FORMAT(TUMBLE_START(rowtime, INTERVAL '10' SECOND ),'yyyy-MM-dd HH:mm:ss') stt, " +
                            "   DATE_FORMAT(TUMBLE_END(rowtime, INTERVAL '10' SECOND ),'yyyy-MM-dd HH:mm:ss') edt , " +
                            "   province_id," +
                            "   province_name," +
                            "   province_area_code area_code," +
                            "   province_iso_code iso_code," +
                            "   province_3166_2_code iso_3166_2 ," +
                            "   sum(split_total_amount) order_amount," +
                            "   COUNT(DISTINCT order_id) order_count, " +
                            "   UNIX_TIMESTAMP()*1000 ts " +
                            "from  order_wide " +
                            "group by  " +
                            "   TUMBLE(rowtime, INTERVAL '10' SECOND ), " +
                            "   province_id," +
                            "   province_name," +
                            "   province_area_code," +
                            "   province_iso_code," +
                            "   province_3166_2_code ");
```

该方法目前存在版本不兼容问题，故不使用该方法

### 实际采用动态表转流写入

​	由于FlinkSql的运行环境与流式api不同，所以先为其创建对应的baseApp-BaseSqlApp

​	不同点是需要调用流式api的create方法创建表api

```java
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
```

​	建立一个动态表A, 与source关联(Kafka的topic)

​	调用tEnv的tEnv.executeSql(）方法

```java
tEnv.executeSql(“sql”）
```

​	在这个动态A表上执行连续查询	

​	调用tEnv的sqlQuery方法

```java
tEnv
                .sqlQuery(“sql”）
```

​	把上面连续查询的结果写入到动态表B中
​    	//tEnv.executeSql("insert into province_stats_2021 select * from " + resultTable);
​    	// 由于连接器与flink版本不兼容, 需要把动态表的数据转成流之后再写入

调用tEnv的toRetractStream

```java
 tEnv
                .toRetractStream(resultTable, ProvinceStats.class)
                .filter(t -> t.f0)  // 把撤回的过滤掉
                .map(t-> t.f1)
                .addSink(FlinkSinkUtil.getClickhouseSink("gmall2021", "province_stats_2021", ProvinceStats.class));
```

##  4.4 搜索关键词主题宽表

### 需求分析与思路

关键词主题为了大屏展示中字符云的展示效果，用于感性感知目前的用户更关心的商品和关键词。

是维度聚合的结果，根据聚合结果决定关键词的大小

### 数据来源

用户搜索和商品主题统计中获取关键词

​	用户搜索关键词可能只是包含在其中的一部分，涉及数据切割，切词

​	用户搜索主要来自页面日志

### 具体实现

#### 	在ClickHouse中建表

​	选择需要的字段

```sql
use gmall2021;
create table keyword_stats_2021 (
    stt DateTime,
    edt DateTime,
    keyword String ,
    source String ,
    ct UInt64 ,
    ts UInt64
)engine =ReplacingMergeTree( ts)
        partition by  toYYYYMMDD(stt)
        order by  ( stt,edt,keyword,source );
```

​	使用FlinkSql从pageLog中读字段

​		能读取的字段有三个 common， page， ts

```json
{
    "common":{
        "ar":"310000",
        "uid":"40",
        "os":"iOS 13.2.3",
        "ch":"Appstore",
        "is_new":"0",
        "md":"iPhone Xs Max",
        "mid":"mid_2",
        "vc":"v2.0.1",
        "ba":"iPhone"
    },
    "page":{
        "page_id":"",
        "during_time":18411,
        "last_page_id":""
    },
    "ts":1614602604000
}
```

​	建立动态表，与source相关联（kafka的topic）

```sql
tEnv.executeSql("create table page_log(" +
                " common map<string, string>, " +
                " page map<string, string>, " +
                " ts bigint, " +
                " et as to_timestamp(from_unixtime(ts/1000)), " + // 先把long转成yyyy-MM-dd HH:mm:ss , 然后再转成时间戳
                " watermark for  et as et - interval '3' second" +
                ")with(" +
                "   'connector' = 'kafka'," +
                "   'properties.bootstrap.servers' = 'hadoop162:9092,hadoop163:9092,hadoop164:9092'," +
                "   'properties.group.id' = 'DwsKeywordStatsApp'," +
                "   'topic' = '" + Constant.TOPIC_DWD_PAGE_LOG + "'," +
                "   'scan.startup.mode' = 'latest-offset'," +
                "   'format' = 'json'" +
```

其中添加水印

```sql
" watermark for  et as et - interval '3' second"
```

​	计算每个词的热度（搜索次数）

​		在pagelog中的搜索内容是itemId对应的关键词

​		itemId不为空，pageId为goodList

```java
Table t1 = tEnv.sqlQuery("select" +
                " page['item'] kw, " +
                " et " +
                "from page_log " +
                "where page['item'] is not null " +
                "and page['page_id']='good_list'");
```

​	搜索词举例

```java
/*
小米手机
华为手机
苹果手机
oppo手机
小米
手机
华为
手机
苹果
手机
oppo
手机
---
再统计每个词的搜索次数
```

----
#### 自定义函数

​    1. 标量函数
​    2. 表值函数
​    3. 聚合函数
​    4. AggregateTable 聚合表值函数

 使用: 表值函数 table

​	注册自定义函数

调用表环境的createTemporaryFunction方法

```java
tEnv.createTemporaryFunction("ik_analyzer", KeyWordUdtf.class);
```

执行sql

制表函数和自定义函数的连接方法

​	内连接 lateral table 传入搜索词字段

```java
Table t2 = tEnv.sqlQuery("select" +
                " kw, " +
                " et, " +
                " word " +
                " from t1 " +
                " join lateral table(ik_analyzer(kw)) on true");
        tEnv.createTemporaryView("t2", t2);
```

