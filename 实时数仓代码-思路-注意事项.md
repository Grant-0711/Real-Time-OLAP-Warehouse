# Idea Project

https://github.com/Grant-0711/Real-Time-OLAP-Warehouse/tree/main/gmall2021-parent

# DWD层: 用户行为日志

## 为常用的模块创建工具类

### **FlinkSourceUtil（获取flinkSource）**

一个获取kafka source的静态方法：

​	kafka地址

​	指定消费者组

​	指定消费的topic

返回值是一个FlinkKafkaConsumer<T>，其中需要指定：

​	topic

​	schema：new SimpleStringSchema()

​	properties

properties中需要指定：

​	地址

​	消费者组

​	offset auto.offset.reset：latest/from-beginning

​	隔离级别isolation.level ：read_committed 是读取事务已经提交的

```java
public class FlinkSourceUtil {
    public static FlinkKafkaConsumer<String> getKafkaSource(String groupId, String topic){
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("group.id", groupId);
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("isolation.level", "read_committed");

        return new FlinkKafkaConsumer<String>(
                topic,
                new SimpleStringSchema(),
                props
```



### **Constant（放置常用的常量）**

```java
public class Constant {
    public static final String TOPIC_ODS_LOG = "ods_log";
    public static final String TOPIC_DWD_START_LOG = "dwd_start_log";
    public static final String TOPIC_DWD_PAGE_LOG = "dwd_page_log";
    public static final String TOPIC_DWD_DISPLAY_LOG = "dwd_display_log";
    public static final String TOPIC_ODS_DB = "ods_db";
```



### **BaseAppV1（获取运行环境以及基本配置）**

子类继承该类，实现具体业务

初始化方法创建运行环境，配置相关参数，例如

​	HADOOP_USER_NAME

​	设置精准一次性保证（默认）

​	Checkpoint必须在一分钟内完成，否则就会被抛弃

​	开启在 job 中止后仍然保留的 externalized checkpoints

​	设置状态后端

通过StreamExecutionEnvironment.getExecutionEnvironment()创建运行环境

通过env.addSource(MyKafkaUtil.getKafkaSource(groupId, topic))创建数据流



run方法获取运行环境和数据流，是一个抽象方法，子类重写，实现业务。

```java
public abstract class BaseAppV1 {
    public abstract void run(StreamExecutionEnvironment env, DataStreamSource<String> sourceStream );

    public void init(int port,
                     int p,
                     String ck,
                     String groupId,
                     String topic) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", port);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(p);

        // 设置状态后端
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop162:8020/flink-realtime/ck/" + ck);

        env.enableCheckpointing(3000, CheckpointingMode.EXACTLY_ONCE);

        env.getCheckpointConfig().setCheckpointTimeout(60 * 1000);
        env.getCheckpointConfig()
                .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 具体的业务
        DataStreamSource<String> sourceStream = env
                .addSource(FlinkSourceUtil.getKafkaSource(groupId, topic));

//        sourceStream.print();  // 不同的应用有不同的业务
        run(env, sourceStream);
```

## DWDLogApp具体实现

首先要继承BaseAppV1，重写run方法。由于BaseAppV1中init方法不是静态的，所以要想在main方法中调用init，需要先创建DWDLogApp对象。

在创建对象时传入不同的参数

### 主要任务

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

### 实现思路

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

​	1.考虑数据乱序，因此采用eventTime

​	2.添加窗口

​		每个用户生命周期中的第一个窗口中才会有可能设置为新用户

​		按照eventTime排序，最小的应该是新用户记录，其他就是旧用户

​	3.如何确定第一个窗口

​		使用状态记录



ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据