# Idea Project

https://github.com/Grant-0711/Real-Time-OLAP-Warehouse/tree/main/gmall2021-parent

# DWD层: 用户行为日志

## 为常用的模块创建工具类

### **FlinkSourceUtil（获取flinkSource）**

一个获取kafka source的静态方法：

​	kafka地址

​	指定消费者组

​	指定消费的topic

返回值是一个FlinkKafkaConsumer<T>，其中需要指定：

​	topic

​	schema：new SimpleStringSchema()

​	properties

properties中需要指定：

​	地址

​	消费者组

​	offset auto.offset.reset：latest/from-beginning

​	隔离级别isolation.level ：read_committed 是读取事务已经提交的

```java
public class FlinkSourceUtil {
    public static FlinkKafkaConsumer<String> getKafkaSource(String groupId, String topic){
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("group.id", groupId);
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("isolation.level", "read_committed");

        return new FlinkKafkaConsumer<String>(
                topic,
                new SimpleStringSchema(),
                props
```



### **Constant（放置常用的常量）**

```java
public class Constant {
    public static final String TOPIC_ODS_LOG = "ods_log";
    public static final String TOPIC_DWD_START_LOG = "dwd_start_log";
    public static final String TOPIC_DWD_PAGE_LOG = "dwd_page_log";
    public static final String TOPIC_DWD_DISPLAY_LOG = "dwd_display_log";
    public static final String TOPIC_ODS_DB = "ods_db";
```



### **BaseAppV1（获取运行环境以及基本配置）**

子类继承该类，实现具体业务

初始化方法创建运行环境，配置相关参数，例如

​	HADOOP_USER_NAME

​	设置精准一次性保证（默认）

​	Checkpoint必须在一分钟内完成，否则就会被抛弃

​	开启在 job 中止后仍然保留的 externalized checkpoints

​	设置状态后端

通过StreamExecutionEnvironment.getExecutionEnvironment()创建运行环境

通过env.addSource(MyKafkaUtil.getKafkaSource(groupId, topic))创建数据流



run方法获取运行环境和数据流，是一个抽象方法，子类重写，实现业务。

```java
public abstract class BaseAppV1 {
    public abstract void run(StreamExecutionEnvironment env, DataStreamSource<String> sourceStream );

    public void init(int port,
                     int p,
                     String ck,
                     String groupId,
                     String topic) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", port);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(p);

        // 设置状态后端
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop162:8020/flink-realtime/ck/" + ck);

        env.enableCheckpointing(3000, CheckpointingMode.EXACTLY_ONCE);

        env.getCheckpointConfig().setCheckpointTimeout(60 * 1000);
        env.getCheckpointConfig()
                .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 具体的业务
        DataStreamSource<String> sourceStream = env
                .addSource(FlinkSourceUtil.getKafkaSource(groupId, topic));

//        sourceStream.print();  // 不同的应用有不同的业务
        run(env, sourceStream);
```

### BaseAppV2

与V1的区别是传入多个topic，以可变长参数的形式

#### 操作区别

用一个array存topic

```java
        ArrayList<String> topics = new ArrayList<>(Arrays.asList(otherTopics));
        topics.add(topic);
```

对于array中的每一个topic都调用运行环境的addSource方法

```java
HashMap<String, DataStreamSource<String>> topicAndStreamMap = new HashMap<>();
        for (String t : topics) {
            DataStreamSource<String> stream = env.addSource(FlinkSourceUtil.getKafkaSource(groupId, t));
            topicAndStreamMap.put(t, stream);
```

### FlinkSinkUtil

```java
public class FlinkSinkUtil {
    // 得到一个kafka source
    public static FlinkKafkaProducer<String> getKafkaSink(String topic) {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                topic,
                (KafkaSerializationSchema<String>) (element, timestamp) -> new ProducerRecord<>(topic, null, element.getBytes(StandardCharsets.UTF_8)),
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                    element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
        );
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
    }
```

### JDBCUtil

封装getJdbcConnection方法，传入driver类和url

```java
 public static Connection getJdbcConnection(String driver,
                                               String url) throws ClassNotFoundException, SQLException {
        Class.forName(driver);
        return DriverManager.getConnection(url);
    }

    public static void main(String[] args) throws Exception {
        Connection conn = getJdbcConnection(Constant.MYSQL_DRIVER, Constant.MYSQL_URL);
        List<JSONObject> list = queryList(conn, "select * from user_info where id='1'", null, JSONObject.class);
        for (JSONObject obj : list) {

            System.out.println(obj);
        }
    }

    // 执行指定sql语句, 并把查询到的结果封装到List集合中
    public static <T> List<T> queryList(Connection conn,
                                        String sql,
                                        Object[] args,
                                        Class<T> tClass) throws Exception {
        PreparedStatement ps = conn.prepareStatement(sql);
        // 1. 先把sql中的占位符进行赋值  args 的长度是几就表示sql中有几个占位符
        for (int i = 0; args != null && i < args.length; i++) {
            ps.setObject(i + 1, args[i]);
        }

        ArrayList<T> result = new ArrayList<>();
        // 2. 执行sql语句
        ResultSet resultSet = ps.executeQuery();
        // 通过 resultSet 获取相关的元数据类得到
        ResultSetMetaData metaData = resultSet.getMetaData();
        while (resultSet.next()) {
            // 3. 遍历到每行数据, 把这些数据封装到 T 类型的对象中
            T t = tClass.newInstance();  // 3.1 利用反射的方式, 创建 t类型的对象
            // 知道属性名和属性只  setAge(10)
            // 遍历每一列
            for (int i = 0; i < metaData.getColumnCount(); i++) {
                String columnName = metaData.getColumnLabel(i + 1);// 列的索引是从1开始
                Object value = resultSet.getObject(columnName);
                BeanUtils.setProperty(t, columnName, value);
            }
            result.add(t);
        }

        return result;
    }
```

### DimUtil

```java
// 从Phoenix读取维度数据
    public static JSONObject readDimFromPhoenix(Connection conn,
                                                String table,
                                                Object id) throws Exception {
        String sql = "select * from " + table + " where id=?";

        // 可以执行各种sql, 查询的结果应该会有多行
        List<JSONObject> list = JDBCUtil.queryList(conn, sql, new Object[]{id.toString()}, JSONObject.class);

        if (list != null && list.size() > 0) {
            return list.get(0);
        }
        return new JSONObject();

    }

    // 动缓存读取维度数据
    public static JSONObject readDimFromCache(Jedis client,
                                              String table,
                                              Object id) {

        String key = table + ":" + id;
        String jsonStr = client.get(key);

        JSONObject dim = null;
        if (jsonStr != null) {
            dim = JSON.parseObject(jsonStr);
            // 每读一个维度, 应该重新计算一个24小时的过期时间   更新过期时间
            client.expire(key, Constant.DIM_EXPIRE_SECOND);
        }
        return dim;
    }

    // 把维度数据存入缓存中
    private static void saveDimToCache(Jedis client,
                                       String table,
                                       Object id,
                                       JSONObject dim) {

        String key = table + ":" + id;
        String value = dim.toJSONString();

        client.setex(key, Constant.DIM_EXPIRE_SECOND, value);  // 把维度写入到redis中
    }

    public static JSONObject readDim(Connection conn,
                                     Jedis client,
                                     String table,
                                     Object id) throws Exception {
        client.select(1); // 把数据保存1号库

        // 首先去缓存中读取, 缓存没有在从hbase中读取
        JSONObject dim = readDimFromCache(client, table, id);
        if (dim == null) {  // 缓存中没有数据
            System.out.println("从 Phoenix  读取维度:" + table + "  " + id);
            dim = readDimFromPhoenix(conn, table, id);
            // 把这次读到的dim数据存储到缓存中. 否则缓存中永远没有数据
            saveDimToCache(client, table, id, dim);
        } else {
            System.out.println("从 缓存  读取维度: " + table + "  " + id);
        }

        return dim;
    }

    public static void main(String[] args) throws Exception {
        Connection conn = JDBCUtil.getJdbcConnection(Constant.PHOENIX_DRIVER, Constant.PHOENIX_URL);
        JSONObject object = readDimFromPhoenix(conn, "dim_user_info", 50000000);
        System.out.println(object);
    }
```



## DWDLogApp具体实现

首先要继承BaseAppV1，重写run方法。由于BaseAppV1中init方法不是静态的，所以要想在main方法中调用init，需要先创建DWDLogApp对象。

在创建对象时传入不同的参数

### 主要任务

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

### 实现思路

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

​	1.考虑数据乱序，因此采用eventTime

​	2.添加窗口

​		每个用户生命周期中的第一个窗口中才会有可能设置为新用户

​		按照eventTime排序，最小的应该是新用户记录，其他就是旧用户

​	3.如何确定第一个窗口

​		使用状态记录



ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

​	主流

```java
// 把启动日志写入到主流中
                            out.collect(input);
```

​	侧流输出

```java
// 1. 如果是页面
                            JSONObject page = input.getJSONObject("page");
                            if (page != null) {
                                ctx.output(pageTag, input);
                            }
                            // 2. 如果曝光
                            JSONArray displays = input.getJSONArray("displays");
                            if (displays != null) {
                                for (int i = 0; i < displays.size(); i++) {
                                    JSONObject display = displays.getJSONObject(i);
                                    // 把一些其他信息插入到display中
                                    display.put("ts", input.getLong("ts"));
                                    display.put("page_id", input.getJSONObject("page").getString("page_id"));

                                    display.putAll(input.getJSONObject("common"));

                                    ctx.output(displayTag, display);
```

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

# DWD层: 业务数据

业务数据的来源是由MaxWell从mysql采集得到

但是MaxWell把全部数据统一写入一个Topic, 包括业务数据，也包含维度数据，不利于日后的数据处理

所以DWD业务数据层从Kafka的业务数据ODS层读取数据，经过处理后，将**维度数据保存到Hbase**，将**事实数据写回Kafka**作为业务数据的**DWD层**。

## 主要任务

接受kafka数据并且过滤空值

实现动态分流

​	实现策略是将动态配置方案存入mysql中

​	视图

https://www.processon.com/diagraming/60dcbdc5e0b34d238be0a17e

## 具体实现

### 设计动态配置表

将配置表封装成java实体类

```java
public class TableProcess {
    //动态分流Sink常量
    public static final String SINK_TYPE_HBASE = "hbase";
    public static final String SINK_TYPE_KAFKA = "kafka";
    public static final String SINK_TYPE_CK = "clickhouse";
    //来源表
    private String sourceTable;
    //操作类型 insert,update,delete
    private String operateType;
    //输出类型 hbase kafka
    private String sinkType;
    //输出表(主题)
    private String sinkTable;
    ---略
```

### Flink从kafka读取数据

### Flink从mysql读取配置（CDC）

#### Flink SQL CDC

CDC 全称是 **Change Data Capture** ，捕获变更数据

#### Flink SQL内置debeizum

业界主要有基于查询的 CDC 和基于日志的 CDC 

#### 查询CDC和日志CDC对比（重点）

|                          | 查询CDC                                      | 日志CDC                                |
| ------------------------ | :------------------------------------------- | -------------------------------------- |
| 概念                     | 每次捕获变更都发起查询，全表扫表过滤变更数据 | 读取数据库log保持监控 例如mysql binlog |
| 开源产品                 | sqoop kafka jdbc source                      | canal maxwell debezium                 |
| 执行模式                 | batch                                        | streaming                              |
| 捕获所有变化             | 不可以                                       | 可                                     |
| 低延迟，负载小           | 不可以                                       | 可                                     |
| 不侵入业务（lastupdate） | 不可以                                       | 可                                     |
| 捕获删除和旧的           | 不可以                                       | 可                                     |
| 捕获旧的状态             | 不可以                                       | 可                                     |



Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取**全量数据**和**增量变更数据**的 source 组件

<https://github.com/ververica/flink-cdc-connectors>

#### 代码构建

①读取配置表的数据（flink-sql）

​		获取tableStream运行环境

```java
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
```

​		封装一个方法读取配置表的数据

​		创建一个临时表来存配置表的数据

```java
tEnv.executeSql("CREATE TABLE `table_process` (\n" +
				略字段名
                "  PRIMARY KEY (`source_table`,`operate_type`) not enforced" +
                ")with(" +
                " 'connector' = 'mysql-cdc',\n" +
				略配置信息
                ")");
```

​		将查询到的数据临时表转换为流，流的数据类型用封装的样例类

​		对流数据进行过滤

```java
return tEnv
                .toRetractStream(tpTable, TableProcess.class)
                .filter(t -> t.f0)
                .map(t -> t.f1);
```

​		

②对sourceStream数据做一些ETL

​		map ------> filter

```java
                .map(JSON::parseObject)
                .filter(obj ->
                        obj.getString("database") != null
                                && obj.getString("table") != null
                                && obj.getString("type") != null
                                && (obj.getString("type").contains("insert") || obj.get("type").equals("update"))
                                && obj.getString("data") != null
                                && obj.getString("data").length() >2
```



#### 注意事项

##### 读取mysql模式的问题

​	snapshot.mode

​		用来控制debezium如何从mysql读数据

​		参数 initial 是程序一启动则首先把表中所有数据读出（通过mysql查询）

​		基于binlog来监控新增和变化

​		参数 never 只是监控变化，不监控原始数据

​	指定格式

```
debezium.snapshot.mode =  initial or never 
```



##### Maxwell bootstrap

旧的数据需要通过bootstrap的方式读取，导致obj.getString("type")为bootstrap insert 还会包含bootstrap complete，因此在过滤时要考虑这个问题

### 将配置做成广播流与业务数据进行connect，动态控制业务数据的sink方向

#### 	配置流做成广播流

​		将查询到的配置数据临时表转换为流对象之后，调用流对象的broadcast方法，其中要传入一个MapStateDescriptor<Object, Object>，也就是说广播流中的数据类型是map，key和value的类型需要决定，依据表名和操作类型来确定sink

```java
MapStateDescriptor<String, TableProcess> tpStateDesc = new MapStateDescriptor<>("tpState",
               String.class,
                TableProcess.class);
                
        BroadcastStream<TableProcess> tpBCstream = tpStream.broadcast(tpStateDesc);
```

#### 	connect两个流

​	调用数据流的connect方法连接两个流

​	然后调用process方法对数据流的数据和广播流的数据进行处理

```java
return etlStream.connect(tpBCstream)
                .process(new BroadcastProcessFunction<JSONObject,
                        TableProcess, Tuple2<JSONObject,TableProcess>>() {
                    @Override
                    public void processElement(JSONObject jsonObject,
                                               ReadOnlyContext readOnlyContext,
                                               Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        ReadOnlyBroadcastState<String, TableProcess> tpState = readOnlyContext.getBroadcastState(tpStateDesc);
                        String key = jsonObject.getString("table") + ":" + jsonObject.getString("type")
                                .replaceAll("bootstrap-", "");

                        TableProcess tableProcess = tpState.get(key);

                        if (tableProcess != null) {
                            collector.collect(Tuple2.of(jsonObject,tableProcess));
                        }
                    }

                    @Override
                    public void processBroadcastElement(TableProcess tableProcess,
                                                        Context context,
                                                        Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        //获取广播状态
                        BroadcastState<String, TableProcess> TPstate = context.getBroadcastState(tpStateDesc);
                        //把配置写入广播状态
                        String key = tableProcess.getSourceTable() + ":" + tableProcess.getOperateType();
                        TPstate.put(key,tableProcess);
                    }
                });
```

​	其中广播流时把表名和类型写入广播流，数据流是读取广播状态中的数据，并和数据以Tuple2的形式写出

#### 	数据写出到不同的流

​		封装了dynamicSplitStream方法，传入的参数是SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> dataTpStream也就是添加了广播状态之后的数据流

```java
                .process(new ProcessFunction<Tuple2<JSONObject, TableProcess>, Tuple2<JSONObject, TableProcess>>() {
                    @Override
                    public void processElement(Tuple2<JSONObject, TableProcess> value,
                                               Context ctx,
                                               Collector<Tuple2<JSONObject, TableProcess>> out) throws Exception {

                        // 去kafka放入主流  去hbase的放入侧输出流
                        TableProcess tp = value.f1;
                        //根据配置表中sinkcolum的值对数据进行一些过滤

                        // 1. 根据配置表中sink_columns的值, 数据做一些过滤   100  50需要sink, 把不需要sink的列删除
                        JSONObject data = value.f0.getJSONObject("data");
                        filterColumns(data, tp);

                        // 写入到kafka或者hbase的数据应该只有data中的数据, 不应包含其他的一些元数据了
                        Tuple2<JSONObject, TableProcess> result = Tuple2.of(data, tp);
                        if (TableProcess.SINK_TYPE_KAFKA.equals(tp.getSinkType())) {
                            //                        out.collect(value);
                            out.collect(result);
                        } else if (TableProcess.SINK_TYPE_HBASE.equals(tp.getSinkType())) {
                            // 把到hbase的数据写入到测输出流
                            ctx.output(new OutputTag<Tuple2<JSONObject, TableProcess>>("hbase") {}, result);
```

​		在sink之前数据要根据在配置表中sinkColumn列中指定的字段对数据进行过滤，排除掉不需要的字段，封装了filterColumns(JSONObject data, TableProcess tp)方法

```java
private void filterColumns(JSONObject data, TableProcess tp) {
                        // id,user_id,sku_id,cart_price,sku_num,img_url,sku_name,is_checked,create_time,operate_time,is_ordered,order_time,source_type,source_id
                   /* Set<String> keys = data.keySet();
                    Iterator<String> it = keys.iterator();
                    while (it.hasNext()) {
                        // 如果需要删应该在这个地方删
                    }*/
                        List<String> cs = Arrays.asList(tp.getSinkColumns().split(","));
                        data.keySet().removeIf(key -> !cs.contains(key));
```

#### 不同流的数据写出到不同的sink		

##### **Kafka**

调用流的addSink方法，要传入对应的sinkFunction，这里采用自定义的方法

返回值的类型需要是FlinkKafkaProducer的有参构造方法，在其中传入topic，KafkaSerializationSchema，kafka Properties and Semantic

由于写出的数据要进入不同的topic，所以要重载FlinkSinkUtil中的getKafkaSink方法

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                     element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
```

##### **Hbase**

使用Phoenix写入

​	Phoenix中的表如何创建

​		动态建表

​		建表时机

​			第一条数据来时去建对应维度表

​	向Phoenix写入数据

​		动态写入

​		sql语句根据不同表拼出不同sql

###### 具体实现

对应流调用addSink方法，传入的是自定义工具类FlinkSinkUtil的getHbaseSink方法

可以进行分组，使用keyby，按照SinkTable名进行keyby，提高写入效率

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
```

封装的类PhoenixSink()

自定义sink：继承类RichSinkFunction

```java
public class PhoenixSink extends RichSinkFunction<Tuple2<JSONObject, TableProcess>> {
    private Connection conn;
    private ValueState<Boolean> createTableState;
```

重写open方法，建立Phoenix连接。使用标准的jdbc连接

​			加载驱动（常见数据库不用加载，小众数据库需要加载。例如Phoenix）

​			获取连接对象

```java
 @Override
    public void open(Configuration parameters) throws Exception {
        // 0. 建立到Phoenix的连接. 使用标准的jdbc连接就可以了
        // 1. 加载驱动(常见的数据库可以不用加载, java会根据url自动的加载, 有些数据库比较加载, 比如: Phoenix)
        Class.forName(Constant.PHOENIX_DRIVER);
        // 2. 获取连接对象
        conn = DriverManager.getConnection(Constant.PHOENIX_URL);

        createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

**重写invoke方法，是用来实现对数据的操作业务**

​	先检测表是否存在，如果不存在，创建不存在的表

​	建表：执行sql（建表）来实现在Phoenix中建表

​	1.先拼接一个建表语句

​	2.执行sql

​		先获取连接对象

​		连接对象调用prepareStatement方法（得到预处理语句，传入sql）

​		所有字段都使用varchar数据类型

​		对sql占位符进行赋值

​		执行sql，调用execute方法，之后调用connect的commit方法，最后关闭

```java
@Override
    public void invoke(Tuple2<JSONObject, TableProcess> value,
                       Context context) throws Exception {
        // 1. 检测表是否存在, 如果不存在需要先建表
        checkTable(value);
        // 2.  把这条数据写入到Phoenix中
        write2Hbase(value); }
    private void write2Hbase(Tuple2<JSONObject, TableProcess> value) throws SQLException {
        JSONObject data = value.f0;
        TableProcess tp = value.f1;
        // upsert into user(id, name, age) values(?,?,?)

        StringBuilder sql = new StringBuilder();
        String[] cs = tp.getSinkColumns().split(",");
        sql
                .append("upsert into ")
                .append(tp.getSinkTable())
                .append("(")
                .append(tp.getSinkColumns())
                .append(")values(");
        // 添加占位符 TODO
        // sql.append(tp.getSinkColumns().replaceAll("[^,]+", "?"));  // 正则替换
        for (String c : cs) {
            sql.append("?,");
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")");
        PreparedStatement ps = conn.prepareStatement(sql.toString());
        // 给占位符赋值
        for (int i = 0; i < cs.length; i++) {
            String v = data.get(cs[i]) == null ? "" : data.get(cs[i]).toString();  // 把所有的值变成string,然后就和Phoenix中的varchar对应了  "null" null
            ps.setString(i + 1, v);
        }
        ps.execute();
        conn.commit();
        ps.close();
    }
    // 检测表, 并创建不存在的表
    private void checkTable(Tuple2<JSONObject, TableProcess> value) throws SQLException, IOException {
        // 执行建表语句,实现在Phoenix中完成建表
        if (createTableState.value() == null) {  // 可以避免每来一条数据都需要去执行一次sql
            TableProcess tp = value.f1;
            // 1. 先拼接一个建表语句  TODO
            // create table if not exists t(name varchar, age varchar, constraint pk primary key(name, age))
            StringBuilder sql = new StringBuilder();
            sql
                    .append("create table if not exists ")
                    .append(tp.getSinkTable())
                    .append("(");
        /*for (String c : tp.getSinkColumns().split(",")) {
            sql.append(c).append(" varchar,");
        }
        sql.deleteCharAt(sql.length() - 1); // 去掉最后一个逗号*/
            sql
                    .append(tp.getSinkColumns().replaceAll(",", " varchar,"))
                    .append(" varchar, constraint pk primary key(");
            // 拼接主键
            sql.append(tp.getSinkPk() == null ? "id" : tp.getSinkPk());
            sql
                    .append("))")
                    .append(tp.getSinkExtend() == null ? "" : tp.getSinkExtend());

            // 2. 执行sql语句
            // 2.1 得到预处理语句
            System.out.println("建表语句: " + sql.toString());

            PreparedStatement ps = conn.prepareStatement(sql.toString());
            // 2.2 给sql中的占位符进行赋值 不需要给占位符赋值, 因为没有问号
            // 2.3 执行sql
            ps.execute();
            conn.commit();
            ps.close();
            createTableState.update(true);}}
```

重写close方法

```java
    @Override
    public void close() throws Exception {
        // 是否资源
        if (conn != null) {
            conn.close();
```

###### 动态建表的优化

添加一个状态来判断是不是某个表的第一条数据，是的话再创建表，不用每次都查询Phoenix是否存在表

```java
createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

##### 盐表

Phoenix Salted Table是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。

通过在创建表的时候指定SALT_BUCKETS来实现pre-split(预分割)。

###### 实现原理

将散列取余后byte值插入owkey第一个字节，通过定义region的start key 和 end key 将数据分割到不同的region

以此来防止自增序列引入的热点问题，从而达到平衡HBase集群的读写性能的目的。
salted byte的计算方式大致如下

hash(rowkey) % SALT_BUCKETS

SALT_BUCKETS的取值为1到256

默认下salted byte将作为每个region的start key 及 end key，以此分割数据到不同的region，这样能做到具有相同salted byte的数据能够位于同一个region里面

#### 数据写入kafka细节

由于数据量小的情况下写入并不是真正的轮询，而是分时间片写入，一段时间内写入一个分区，有可能会造成数据倾斜，解决方法可以指定一个key为null或者手动指定分区，数据量大时不会有这个问题

#### 动态分流的局限性

##### 配置数据流和数据流的到达时间不匹配问题

解决方法

①先启动配置数据流，再启动数据流

②如果还是没有等到配置流

可能的情况：

​	真的没有配置

​	配置未到

​		把配置先存入集合

​		规定时间后再去单独处理（自定义定时器）

##### CDC如何处理采集数据时读锁问题

maxwell和debezium独有的读取旧数据的功能

​	在读取的时候如果有业务数据在写，会出现数据不一致的情况

​	在读取时别的进程没有权限写，等读完才允许写

解决

①关闭读锁，不建议，一般会开启

# DWM层

DWM层的指标和DWS层挂钩

对数据做轻度聚合，例如过滤，例如指存某个用户当天的第一条数据

## 需求概述

| 统计主题 | 需求指标       | 输出方式   | 计算来源                 | 来源层级 |
| -------- | -------------- | ---------- | ------------------------ | -------- |
| 访客     | pv             | 可视化大屏 | page_log直接可求         | dwd      |
|          | uv             | 可视化大屏 | 需要用page_log过滤去重   | dwm      |
|          | 跳出率         | 可视化大屏 | 需要通过page_log行为判断 | dwm      |
|          | 连续访问页面数 | 可视化大屏 | 需要识别开始访问标识     | dwd      |
|          | 连续访问时长   | 可视化大屏 | 需要识别开始访问标识     | dwd      |
| 商品     | 点击           | 多维分析   | page_log直接可求         | dwd      |
|          | 收藏           | 多维分析   | 收藏表                   | dwd      |
|          | 加入购物车     | 多维分析   | 购物车表                 | dwd      |
|          | 下单           | 可视化大屏 | 订单宽表                 | dwm      |
|          | 支付           | 多维分析   | 支付宽表                 | dwm      |
|          | 退款           | 多维分析   | 退款表                   | dwd      |
|          | 评论           | 多维分析   | 评论表                   | dwd      |
| 地区     | pv             | 多维分析   | page_log直接可求         | dwd      |
|          | uv             | 多维分析   | 需要用page_log过滤去重   | dwm      |
|          | 下单           | 可视化大屏 | 订单宽表                 | dwm      |
| 关键词   | 搜索关键词     | 可视化大屏 | 页面访问日志 直接可求    | dwd      |
|          | 点击商品关键词 | 可视化大屏 | 商品主题下单再次聚合     | dws      |
|          | 下单商品关键词 | 可视化大屏 | 商品主题下单再次聚合     | dws      |

**跳出**

访问单个页面后离开

## UV

### 数据来源

①启动日志（只包括移动设备）

数据量可能会偏小

②页面日志

### 思路

利用Flink的状态

单日范围内对用户去重

第二条清零

### 具体实现

从DWD层读取日志数据在流中，从流中拿到数据

```java
sourceStream
                    .map(JSONObject::parseObject)
```

拿到的数据可能是乱序，需要用事件时间，添加水印

调用assignTimestampsAndWatermarks，传入WatermarkStrategy

```java
.assignTimestampsAndWatermarks(WatermarkStrategy
                            .<JSONObject>forBoundedOutOfOrderness(Duration.ofSeconds(3)))
```

调用withTimestampAssigner指定时间戳的JSON字段

```java
.withTimestampAssigner((obj, ts) -> obj.getLong("ts")))
```

根据mid分组

```java
.keyBy((obj -> obj.getJSONObject("common").getString("mid")))
```

分配窗口，传入窗口处理函数ProcessWindowFunction

设定一个状态来判断是否是该mid第一次访问

如果到了第二天要清空状态

用SimpleDateFormat来获取当天日期

由于设定了事件时间，如果乱序数据，时间戳大的数据不会进入第一个窗口，所以不会触发窗口关闭更不会触发窗口函数的计算

只有进入第一个窗口且设定的第一次访问状态为空才能证明是第一条数据

```java
.process(new ProcessWindowFunction<JSONObject, JSONObject, String, TimeWindow>() {
                        private ValueState<Long> firstVisitState;
                        private SimpleDateFormat df;
                        @Override
                        public void open(Configuration parameters) {
                            firstVisitState = getRuntimeContext()
                                    .getState(new ValueStateDescriptor<>("firstVisitState", Long.class));
 df = new SimpleDateFormat("yyyy-MM-dd");}
                        @Override
                        public void process(String key, Context context, Iterable<JSONObject> elements, Collector<JSONObject> outer) throws Exception {
                            //如果到了第二天，则首先清空状态
                            //今天
                            String today = df.format(context.window().getEnd());
                            String last = df.format(firstVisitState.value() == null? 0L :firstVisitState);
                            if (!today.equals(last)){
                                firstVisitState.clear();}
                            if (firstVisitState.value() == null){
                                List<JSONObject> list = CommonUtil.toList(elements);
                                JSONObject first = Collections.min(list, Comparator.comparing(o -> o.getLong("ts")));
                                outer.collect(first);
                                firstVisitState.update(first.getLong("ts"));
```

## 跳出率

跳出：访问了入口页面之后离开

跳出率的分母是访问了入口的所有记录

### 数据来源

页面日志

### 思路

用CEP来实现

按照mid分组，一个用户的访问记录进入一组

只访问入口（不一定是首页），不访问其他

如何确定入口：没有lastPageId	

增加超时时间，来确保入口之后没有访问其他页面

### 具体实现

获取流

```java
KeyedStream<JSONObject, String> stream = sourceStream
                .map(JSON::parseObject)
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<JSONObject>forBoundedOutOfOrderness(Duration.ofSeconds(3))
                                .withTimestampAssigner((obj, ts) -> obj.getLong("ts")))
                .keyBy(obj -> obj.getJSONObject("common").getString("mid"));
```

定义模式（组合模式）

把多个单个模式组合在一起就是组合模式.  组合模式由一个**初始化模式**(.begin(...))开头

严格连续(严格紧邻)

begin中确定过滤数据条件为没有lastVistPage

next中确定pageId和lastPageId不为空

设定超时时间，测试用5秒

```java
Pattern<JSONObject, JSONObject> pattern = Pattern
                .<JSONObject>begin("entry")
                .where(new SimpleCondition<JSONObject>() {
                    @Override
                    public boolean filter(JSONObject value) throws Exception {
                        String lastPageId = value.getJSONObject("page").getString("last_page_id");
                        return lastPageId == null || lastPageId.length() == 0;}})
                .next("second")
                .where(new SimpleCondition<JSONObject>() {
                    @Override
                    public boolean filter(JSONObject value) throws Exception {
// 如果有入口, 后面又跟着其他的页面,这是正常访问.
                        JSONObject page = value.getJSONObject("page");
                        String pageId = page.getString("page_id");
                        String lastPageId = page.getString("last_page_id");
                    /*return (pageId != null && pageId.length() > 0
                        && lastPageId != null && lastPageId.length() > 0);*/
                        return Strings.isNotEmpty(pageId) && Strings.isNotEmpty(lastPageId);} })
                .within(Time.seconds(5));
```

把模式应用到流上

​	调用CEP.pattern，传入定义的模式和流

```java
PatternStream<JSONObject> ps =  CEP.pattern(stream, pattern);
```

取出匹配到或者超时的数据

调用PatternStream对象的select方法

传入OutputTag 和 

PatternTimeoutFunction

其中重写的两个方法分别对应对超时数据和满足条件的数据做处理，这里将超时数据写出

```java
        SingleOutputStreamOperator<JSONObject> result = ps.select(
                new OutputTag<JSONObject>("jump") {},
                new PatternTimeoutFunction<JSONObject, JSONObject>() {
                    @Override
                    public JSONObject timeout(Map<String, List<JSONObject>> pattern,
                                              long timeoutTimestamp) throws Exception {
                        return pattern.get("entry").get(0);  // 获取超时的数据, 就是我们需要的跳出明细
                    }

                },
                new PatternSelectFunction<JSONObject, JSONObject>() {
                    @Override
                    public JSONObject select(Map<String, List<JSONObject>> pattern) throws Exception {
                        return null;  // 满足模式的正常数据, 对我们来说不需要, 可以直接返回null}});
```

## 订单宽表

### 需求分析与思路

为了减少后期join，提前join得到一张宽表

①实现流与流的join

②join之后的数据读取维度数据，加缓存，加异步

join关系图

https://www.processon.com/diagraming/60de3a26637689510d68ecfd

### Flink中的两种join

interval join 和 window join

### 数据来源

维度数据来自于DIM层，存在Hbase，变化缓慢，不适合存入流

### 具体实现

#### ①实现流与流的join

封装join两个事实表的方法，传入两个流，oderInfo和oderDetail

**对流的基本操作**：解析json字符串，解析成对应的表封装类，事件时间，加水印，分组

```java
.get(TOPIC_DWD_ORDER_INFO)
                .map(json -> JSON.parseObject(json, OrderInfo.class))
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                .<OrderInfo>forBoundedOutOfOrderness(Duration.ofSeconds(3))
                                .withTimestampAssigner((info, ts) -> info.getCreate_ts())

                )
                .keyBy(OrderInfo::getId);
```

对这两个流进行interval join

其中join时要规定数据的时间段，例如同一个订单的数据到来不会相隔很久的时间

于是设置between(Time.minutes(-5), Time.minutes(5))

之后调用process方法对其进行处理

```java
orderInfoStream
                .intervalJoin(orderDetailStream)
                .between(Time.minutes(-5), Time.minutes(5))
                // join 完成应该是返回一张宽表, 这张宽表目前维度只有一些id , 其实是缺少一些维度信息
                .process(new ProcessJoinFunction<OrderInfo, OrderDetail, OrderWide>() {
                    @Override
                    public void processElement(OrderInfo left,
                                               OrderDetail right,
                                               Context ctx,
                                               Collector<OrderWide> out) throws Exception {
                        out.collect(new OrderWide(left, right));
```

#### ②join之后的数据读取维度数据，加缓存，加异步

封装join维度信息的方法，传入双流join之后的流

每次来一条数据，通过Phoenix查询一次，join一次

调用双流join的map方法，传入RichMapFunction

​	因为每次来数据join之后数据量不变，因此将数据存入map中

​	在重写的open方法中建立连接Phoenix，连接Phoenix也是使用jdbc，于是封装jdbc类

##### 	在重写的map方法中读取数据

​		封装读取不同表的工具类，基本实现思路是在Phoenix中执行sql，用传参的方式拼成查询不同表的sql

