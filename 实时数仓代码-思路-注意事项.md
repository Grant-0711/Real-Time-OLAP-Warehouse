# Idea Project

https://github.com/Grant-0711/Real-Time-OLAP-Warehouse/tree/main/gmall2021-parent

# DWD层: 用户行为日志

## 为常用的模块创建工具类

### **FlinkSourceUtil（获取flinkSource）**

一个获取kafka source的静态方法：

​	kafka地址

​	指定消费者组

​	指定消费的topic

返回值是一个FlinkKafkaConsumer<T>，其中需要指定：

​	topic

​	schema：new SimpleStringSchema()

​	properties

properties中需要指定：

​	地址

​	消费者组

​	offset auto.offset.reset：latest/from-beginning

​	隔离级别isolation.level ：read_committed 是读取事务已经提交的

```java
public class FlinkSourceUtil {
    public static FlinkKafkaConsumer<String> getKafkaSource(String groupId, String topic){
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("group.id", groupId);
        props.setProperty("auto.offset.reset", "latest");
        props.setProperty("isolation.level", "read_committed");

        return new FlinkKafkaConsumer<String>(
                topic,
                new SimpleStringSchema(),
                props
```



### **Constant（放置常用的常量）**

```java
public class Constant {
    public static final String TOPIC_ODS_LOG = "ods_log";
    public static final String TOPIC_DWD_START_LOG = "dwd_start_log";
    public static final String TOPIC_DWD_PAGE_LOG = "dwd_page_log";
    public static final String TOPIC_DWD_DISPLAY_LOG = "dwd_display_log";
    public static final String TOPIC_ODS_DB = "ods_db";
```



### **BaseAppV1（获取运行环境以及基本配置）**

子类继承该类，实现具体业务

初始化方法创建运行环境，配置相关参数，例如

​	HADOOP_USER_NAME

​	设置精准一次性保证（默认）

​	Checkpoint必须在一分钟内完成，否则就会被抛弃

​	开启在 job 中止后仍然保留的 externalized checkpoints

​	设置状态后端

通过StreamExecutionEnvironment.getExecutionEnvironment()创建运行环境

通过env.addSource(MyKafkaUtil.getKafkaSource(groupId, topic))创建数据流



run方法获取运行环境和数据流，是一个抽象方法，子类重写，实现业务。

```java
public abstract class BaseAppV1 {
    public abstract void run(StreamExecutionEnvironment env, DataStreamSource<String> sourceStream );

    public void init(int port,
                     int p,
                     String ck,
                     String groupId,
                     String topic) {
        System.setProperty("HADOOP_USER_NAME", "atguigu");
        Configuration conf = new Configuration();
        conf.setInteger("rest.port", port);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(p);

        // 设置状态后端
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop162:8020/flink-realtime/ck/" + ck);

        env.enableCheckpointing(3000, CheckpointingMode.EXACTLY_ONCE);

        env.getCheckpointConfig().setCheckpointTimeout(60 * 1000);
        env.getCheckpointConfig()
                .enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 具体的业务
        DataStreamSource<String> sourceStream = env
                .addSource(FlinkSourceUtil.getKafkaSource(groupId, topic));

//        sourceStream.print();  // 不同的应用有不同的业务
        run(env, sourceStream);
```

### FlinkSinkUtil

```java
public class FlinkSinkUtil {
    // 得到一个kafka source
    public static FlinkKafkaProducer<String> getKafkaSink(String topic) {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                topic,
                (KafkaSerializationSchema<String>) (element, timestamp) -> new ProducerRecord<>(topic, null, element.getBytes(StandardCharsets.UTF_8)),
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                    element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
        );
    }
    public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
    }
```



## DWDLogApp具体实现

首先要继承BaseAppV1，重写run方法。由于BaseAppV1中init方法不是静态的，所以要想在main方法中调用init，需要先创建DWDLogApp对象。

在创建对象时传入不同的参数

### 主要任务

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

### 实现思路

①本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

​	1.考虑数据乱序，因此采用eventTime

​	2.添加窗口

​		每个用户生命周期中的第一个窗口中才会有可能设置为新用户

​		按照eventTime排序，最小的应该是新用户记录，其他就是旧用户

​	3.如何确定第一个窗口

​		使用状态记录



ods_log数据中的"is_new"字段

②分流 不同的日志进入不同的流中 

​	主流

```java
// 把启动日志写入到主流中
                            out.collect(input);
```

​	侧流输出

```java
// 1. 如果是页面
                            JSONObject page = input.getJSONObject("page");
                            if (page != null) {
                                ctx.output(pageTag, input);
                            }
                            // 2. 如果曝光
                            JSONArray displays = input.getJSONArray("displays");
                            if (displays != null) {
                                for (int i = 0; i < displays.size(); i++) {
                                    JSONObject display = displays.getJSONObject(i);
                                    // 把一些其他信息插入到display中
                                    display.put("ts", input.getLong("ts"));
                                    display.put("page_id", input.getJSONObject("page").getString("page_id"));

                                    display.putAll(input.getJSONObject("common"));

                                    ctx.output(displayTag, display);
```

③把不同流中的数据sink到不同的topic中，就得到了DWD层数据

# DWD层: 业务数据

业务数据的来源是由MaxWell从mysql采集得到

但是MaxWell把全部数据统一写入一个Topic, 包括业务数据，也包含维度数据，不利于日后的数据处理

所以DWD业务数据层从Kafka的业务数据ODS层读取数据，经过处理后，将**维度数据保存到Hbase**，将**事实数据写回Kafka**作为业务数据的**DWD层**。

## 主要任务

接受kafka数据并且过滤空值

实现动态分流

​	实现策略是将动态配置方案存入mysql中

​	视图

https://www.processon.com/diagraming/60dcbdc5e0b34d238be0a17e

## 具体实现

### 设计动态配置表

将配置表封装成java实体类

```java
public class TableProcess {
    //动态分流Sink常量
    public static final String SINK_TYPE_HBASE = "hbase";
    public static final String SINK_TYPE_KAFKA = "kafka";
    public static final String SINK_TYPE_CK = "clickhouse";
    //来源表
    private String sourceTable;
    //操作类型 insert,update,delete
    private String operateType;
    //输出类型 hbase kafka
    private String sinkType;
    //输出表(主题)
    private String sinkTable;
    ---略
```

### Flink从kafka读取数据

### Flink从mysql读取配置（CDC）

#### Flink SQL CDC

CDC 全称是 **Change Data Capture** ，捕获变更数据

#### Flink SQL内置debeizum

业界主要有基于查询的 CDC 和基于日志的 CDC 

#### 查询CDC和日志CDC对比（重点）

|                          | 查询CDC                                      | 日志CDC                                |
| ------------------------ | :------------------------------------------- | -------------------------------------- |
| 概念                     | 每次捕获变更都发起查询，全表扫表过滤变更数据 | 读取数据库log保持监控 例如mysql binlog |
| 开源产品                 | sqoop kafka jdbc source                      | canal maxwell debezium                 |
| 执行模式                 | batch                                        | streaming                              |
| 捕获所有变化             | 不可以                                       | 可                                     |
| 低延迟，负载小           | 不可以                                       | 可                                     |
| 不侵入业务（lastupdate） | 不可以                                       | 可                                     |
| 捕获删除和旧的           | 不可以                                       | 可                                     |
| 捕获旧的状态             | 不可以                                       | 可                                     |



Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取**全量数据**和**增量变更数据**的 source 组件

<https://github.com/ververica/flink-cdc-connectors>

#### 代码构建

①读取配置表的数据（flink-sql）

​		获取tableStream运行环境

```java
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
```

​		封装一个方法读取配置表的数据

​		创建一个临时表来存配置表的数据

```java
tEnv.executeSql("CREATE TABLE `table_process` (\n" +
				略字段名
                "  PRIMARY KEY (`source_table`,`operate_type`) not enforced" +
                ")with(" +
                " 'connector' = 'mysql-cdc',\n" +
				略配置信息
                ")");
```

​		将查询到的数据临时表转换为流，流的数据类型用封装的样例类

​		对流数据进行过滤

```java
return tEnv
                .toRetractStream(tpTable, TableProcess.class)
                .filter(t -> t.f0)
                .map(t -> t.f1);
```

​		

②对sourceStream数据做一些ETL

​		map ------> filter

```java
                .map(JSON::parseObject)
                .filter(obj ->
                        obj.getString("database") != null
                                && obj.getString("table") != null
                                && obj.getString("type") != null
                                && (obj.getString("type").contains("insert") || obj.get("type").equals("update"))
                                && obj.getString("data") != null
                                && obj.getString("data").length() >2
```



#### 注意事项

##### 读取mysql模式的问题

​	snapshot.mode

​		用来控制debezium如何从mysql读数据

​		参数 initial 是程序一启动则首先把表中所有数据读出（通过mysql查询）

​		基于binlog来监控新增和变化

​		参数 never 只是监控变化，不监控原始数据

​	指定格式

```
debezium.snapshot.mode =  initial or never 
```



##### Maxwell bootstrap

旧的数据需要通过bootstrap的方式读取，导致obj.getString("type")为bootstrap insert 还会包含bootstrap complete，因此在过滤时要考虑这个问题

### 将配置做成广播流与业务数据进行connect，动态控制业务数据的sink方向

#### 	配置流做成广播流

​		将查询到的配置数据临时表转换为流对象之后，调用流对象的broadcast方法，其中要传入一个MapStateDescriptor<Object, Object>，也就是说广播流中的数据类型是map，key和value的类型需要决定，依据表名和操作类型来确定sink

```java
MapStateDescriptor<String, TableProcess> tpStateDesc = new MapStateDescriptor<>("tpState",
               String.class,
                TableProcess.class);
                
        BroadcastStream<TableProcess> tpBCstream = tpStream.broadcast(tpStateDesc);
```

#### 	connect两个流

​	调用数据流的connect方法连接两个流

​	然后调用process方法对数据流的数据和广播流的数据进行处理

```java
return etlStream.connect(tpBCstream)
                .process(new BroadcastProcessFunction<JSONObject,
                        TableProcess, Tuple2<JSONObject,TableProcess>>() {
                    @Override
                    public void processElement(JSONObject jsonObject,
                                               ReadOnlyContext readOnlyContext,
                                               Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        ReadOnlyBroadcastState<String, TableProcess> tpState = readOnlyContext.getBroadcastState(tpStateDesc);
                        String key = jsonObject.getString("table") + ":" + jsonObject.getString("type")
                                .replaceAll("bootstrap-", "");

                        TableProcess tableProcess = tpState.get(key);

                        if (tableProcess != null) {
                            collector.collect(Tuple2.of(jsonObject,tableProcess));
                        }
                    }

                    @Override
                    public void processBroadcastElement(TableProcess tableProcess,
                                                        Context context,
                                                        Collector<Tuple2<JSONObject, TableProcess>> collector) throws Exception {
                        //获取广播状态
                        BroadcastState<String, TableProcess> TPstate = context.getBroadcastState(tpStateDesc);
                        //把配置写入广播状态
                        String key = tableProcess.getSourceTable() + ":" + tableProcess.getOperateType();
                        TPstate.put(key,tableProcess);
                    }
                });
```

​	其中广播流时把表名和类型写入广播流，数据流是读取广播状态中的数据，并和数据以Tuple2的形式写出

#### 	数据写出到不同的流

​		封装了dynamicSplitStream方法，传入的参数是SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> dataTpStream也就是添加了广播状态之后的数据流

```java
                .process(new ProcessFunction<Tuple2<JSONObject, TableProcess>, Tuple2<JSONObject, TableProcess>>() {
                    @Override
                    public void processElement(Tuple2<JSONObject, TableProcess> value,
                                               Context ctx,
                                               Collector<Tuple2<JSONObject, TableProcess>> out) throws Exception {

                        // 去kafka放入主流  去hbase的放入侧输出流
                        TableProcess tp = value.f1;
                        //根据配置表中sinkcolum的值对数据进行一些过滤

                        // 1. 根据配置表中sink_columns的值, 数据做一些过滤   100  50需要sink, 把不需要sink的列删除
                        JSONObject data = value.f0.getJSONObject("data");
                        filterColumns(data, tp);

                        // 写入到kafka或者hbase的数据应该只有data中的数据, 不应包含其他的一些元数据了
                        Tuple2<JSONObject, TableProcess> result = Tuple2.of(data, tp);
                        if (TableProcess.SINK_TYPE_KAFKA.equals(tp.getSinkType())) {
                            //                        out.collect(value);
                            out.collect(result);
                        } else if (TableProcess.SINK_TYPE_HBASE.equals(tp.getSinkType())) {
                            // 把到hbase的数据写入到测输出流
                            ctx.output(new OutputTag<Tuple2<JSONObject, TableProcess>>("hbase") {}, result);
```

​		在sink之前数据要根据在配置表中sinkColumn列中指定的字段对数据进行过滤，排除掉不需要的字段，封装了filterColumns(JSONObject data, TableProcess tp)方法

```java
private void filterColumns(JSONObject data, TableProcess tp) {
                        // id,user_id,sku_id,cart_price,sku_num,img_url,sku_name,is_checked,create_time,operate_time,is_ordered,order_time,source_type,source_id
                   /* Set<String> keys = data.keySet();
                    Iterator<String> it = keys.iterator();
                    while (it.hasNext()) {
                        // 如果需要删应该在这个地方删
                    }*/
                        List<String> cs = Arrays.asList(tp.getSinkColumns().split(","));
                        data.keySet().removeIf(key -> !cs.contains(key));
```

#### 不同流的数据写出到不同的sink		

##### **Kafka**

调用流的addSink方法，要传入对应的sinkFunction，这里采用自定义的方法

返回值的类型需要是FlinkKafkaProducer的有参构造方法，在其中传入topic，KafkaSerializationSchema，kafka Properties and Semantic

由于写出的数据要进入不同的topic，所以要重载FlinkSinkUtil中的getKafkaSink方法

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "hadoop107:9092,hadoop108:9092,hadoop109:9092");
        props.setProperty("transaction.timeout.ms", 15 * 60 * 1000 + ""); // broker要求事务超时时间不能超过15分钟
        return new FlinkKafkaProducer<>(
                "default",
                (KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>) (element, timestamp) -> {
                    String topic = element.f1.getSinkTable();
                    return new ProducerRecord<>(topic,
                            null,                     element.f0.toJSONString().getBytes(StandardCharsets.UTF_8));
                },
                props,
                FlinkKafkaProducer.Semantic.EXACTLY_ONCE
```

##### **Hbase**

使用Phoenix写入

​	Phoenix中的表如何创建

​		动态建表

​		建表时机

​			第一条数据来时去建对应维度表

​	向Phoenix写入数据

​		动态写入

​		sql语句根据不同表拼出不同sql

###### 具体实现

对应流调用addSink方法，传入的是自定义工具类FlinkSinkUtil的getHbaseSink方法

可以进行分组，使用keyby，按照SinkTable名进行keyby，提高写入效率

```java
public static SinkFunction<Tuple2<JSONObject, TableProcess>> getHBaseSink() {
        // 自定义sink: 继承类RichSinkFunction
        return new PhoenixSink();
```

封装的类PhoenixSink()

自定义sink：继承类RichSinkFunction

```java
public class PhoenixSink extends RichSinkFunction<Tuple2<JSONObject, TableProcess>> {
    private Connection conn;
    private ValueState<Boolean> createTableState;
```

重写open方法，建立Phoenix连接。使用标准的jdbc连接

​			加载驱动（常见数据库不用加载，小众数据库需要加载。例如Phoenix）

​			获取连接对象

```java
 @Override
    public void open(Configuration parameters) throws Exception {
        // 0. 建立到Phoenix的连接. 使用标准的jdbc连接就可以了
        // 1. 加载驱动(常见的数据库可以不用加载, java会根据url自动的加载, 有些数据库比较加载, 比如: Phoenix)
        Class.forName(Constant.PHOENIX_DRIVER);
        // 2. 获取连接对象
        conn = DriverManager.getConnection(Constant.PHOENIX_URL);

        createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

**重写invoke方法，是用来实现对数据的操作业务**

​	先检测表是否存在，如果不存在，创建不存在的表

​	建表：执行sql（建表）来实现在Phoenix中建表

​	1.先拼接一个建表语句

​	2.执行sql

​		先获取连接对象

​		连接对象调用prepareStatement方法（得到预处理语句，传入sql）

​		所有字段都使用varchar数据类型

​		对sql占位符进行赋值

​		执行sql，调用execute方法，之后调用connect的commit方法，最后关闭

```java
@Override
    public void invoke(Tuple2<JSONObject, TableProcess> value,
                       Context context) throws Exception {
        // 1. 检测表是否存在, 如果不存在需要先建表
        checkTable(value);
        // 2.  把这条数据写入到Phoenix中
        write2Hbase(value); }
    private void write2Hbase(Tuple2<JSONObject, TableProcess> value) throws SQLException {
        JSONObject data = value.f0;
        TableProcess tp = value.f1;
        // upsert into user(id, name, age) values(?,?,?)

        StringBuilder sql = new StringBuilder();
        String[] cs = tp.getSinkColumns().split(",");
        sql
                .append("upsert into ")
                .append(tp.getSinkTable())
                .append("(")
                .append(tp.getSinkColumns())
                .append(")values(");
        // 添加占位符 TODO
        // sql.append(tp.getSinkColumns().replaceAll("[^,]+", "?"));  // 正则替换
        for (String c : cs) {
            sql.append("?,");
        }
        sql.deleteCharAt(sql.length() - 1);
        sql.append(")");
        PreparedStatement ps = conn.prepareStatement(sql.toString());
        // 给占位符赋值
        for (int i = 0; i < cs.length; i++) {
            String v = data.get(cs[i]) == null ? "" : data.get(cs[i]).toString();  // 把所有的值变成string,然后就和Phoenix中的varchar对应了  "null" null
            ps.setString(i + 1, v);
        }
        ps.execute();
        conn.commit();
        ps.close();
    }
    // 检测表, 并创建不存在的表
    private void checkTable(Tuple2<JSONObject, TableProcess> value) throws SQLException, IOException {
        // 执行建表语句,实现在Phoenix中完成建表
        if (createTableState.value() == null) {  // 可以避免每来一条数据都需要去执行一次sql
            TableProcess tp = value.f1;
            // 1. 先拼接一个建表语句  TODO
            // create table if not exists t(name varchar, age varchar, constraint pk primary key(name, age))
            StringBuilder sql = new StringBuilder();
            sql
                    .append("create table if not exists ")
                    .append(tp.getSinkTable())
                    .append("(");
        /*for (String c : tp.getSinkColumns().split(",")) {
            sql.append(c).append(" varchar,");
        }
        sql.deleteCharAt(sql.length() - 1); // 去掉最后一个逗号*/
            sql
                    .append(tp.getSinkColumns().replaceAll(",", " varchar,"))
                    .append(" varchar, constraint pk primary key(");
            // 拼接主键
            sql.append(tp.getSinkPk() == null ? "id" : tp.getSinkPk());
            sql
                    .append("))")
                    .append(tp.getSinkExtend() == null ? "" : tp.getSinkExtend());

            // 2. 执行sql语句
            // 2.1 得到预处理语句
            System.out.println("建表语句: " + sql.toString());

            PreparedStatement ps = conn.prepareStatement(sql.toString());
            // 2.2 给sql中的占位符进行赋值 不需要给占位符赋值, 因为没有问号
            // 2.3 执行sql
            ps.execute();
            conn.commit();
            ps.close();
            createTableState.update(true);}}
```

重写close方法

```java
    @Override
    public void close() throws Exception {
        // 是否资源
        if (conn != null) {
            conn.close();
```

###### 动态建表的优化

添加一个状态来判断是不是某个表的第一条数据，是的话再创建表，不用每次都查询Phoenix是否存在表

```java
createTableState = getRuntimeContext().getState(new ValueStateDescriptor<Boolean>("createTableState", Boolean.class));
```

##### 盐表

Phoenix Salted Table是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。

通过在创建表的时候指定SALT_BUCKETS来实现pre-split(预分割)。

###### 实现原理

将散列取余后byte值插入owkey第一个字节，通过定义region的start key 和 end key 将数据分割到不同的region

以此来防止自增序列引入的热点问题，从而达到平衡HBase集群的读写性能的目的。
salted byte的计算方式大致如下

hash(rowkey) % SALT_BUCKETS

SALT_BUCKETS的取值为1到256

默认下salted byte将作为每个region的start key 及 end key，以此分割数据到不同的region，这样能做到具有相同salted byte的数据能够位于同一个region里面

#### 数据写入kafka细节

由于数据量小的情况下写入并不是真正的轮询，而是分时间片写入，一段时间内写入一个分区，有可能会造成数据倾斜，解决方法可以指定一个key为null或者手动指定分区，数据量大时不会有这个问题

#### 动态分流的局限性

##### 配置数据流和数据流的到达时间不匹配问题

解决方法

①先启动配置数据流，再启动数据流

②如果还是没有等到配置流

可能的情况：

​	真的没有配置

​	配置未到

​		把配置先存入集合

​		规定时间后再去单独处理（自定义定时器）

##### CDC如何处理采集数据时读锁问题

maxwell和debezium独有的读取旧数据的功能

​	在读取的时候如果有业务数据在写，会出现数据不一致的情况

​	在读取时别的进程没有权限写，等读完才允许写

解决

①关闭读锁，不建议，一般会开启

