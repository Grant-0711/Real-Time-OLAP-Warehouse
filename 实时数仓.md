# **每层职能**

| 分层 | 数据描述                                                     | 生成计算工具        | 存储媒介   |
| ---- | ------------------------------------------------------------ | ------------------- | ---------- |
| ODS  | 原始数据，日志和业务数据                                     | 日志服务器，maxwell | kafka      |
| DWD  | 根据数据对象为单位进行分流，比如订单、页面访问等等。         | FLINK               | kafka      |
| DWM  | 对于部分数据对象进行进一步加工，比如独立访问、跳出行为。依旧是明细数据。 | FLINK               | kafka      |
| DIM  | 维度数据                                                     | FLINK               | HBase      |
| DWS  | 根据某个维度主题将多个事实数据轻度聚合，形成主题宽表。       | FLINK               | Clickhouse |
| ADS  | 把Clickhouse中的数据根据可视化需要进行筛选聚合。             | Clickhouse SQL      | 可视化展示 |

# 数据流向

https://www.processon.com/diagraming/60d5905f5653bb049a48d455

# DWD层: 用户行为日志

作为日志数据的ODS层，从Kafka的ODS层读取的日志数据分为3类: 

页面日志、启动日志和曝光日志

这三类数据虽然都是用户行为数据，但是数据结构不同，需要拆分。拆分后的不同日志写回Kafka不同主题，作为日志DWD层。

## 策略

页面日志输出到**主流**,启动日志输出到启动侧输出流,曝光日志输出到曝光侧输出流



## 1 日志格式

```json
页面日志格式
{
    "common":{
略
    },
    "page":{
略
    },
    "ts":1614575952000
}
---------------------------------------------------------------
启动日志格式
{
    "common":{
略
    },
    "start":{
略
    },
    "ts":1614575950000
}
---------------------------------------------------------------
曝光日志格式
{
    "common":{
略
    },
    "displays":[
        {
            "display_type":"activity",
略
        },
        {
            "display_type":"activity",
略
        },
        {
            "display_type":"promotion",
略
        },
        {
            "display_type":"query",
略
        },
        {
            "display_type":"promotion",
略
        },
        {
            "display_type":"query",
略
        },
        {
            "display_type":"query",
略
        },
        {
            "display_type":"query",
略
        },
        {
            "display_type":"query",
略
        },
        {
            "display_type":"promotion",
略
        },
        {
            "display_type":"query",
略
        }
    ],
    "page":{
        "during_time":8319,
        "page_id":"home"
    },
    "ts":1614575950000
}
```



## 2 主要任务

### 2.1 识别新老客户

本身客户端业务有新老用户的标识，但是不够准确，需要用实时计算再次确认(不涉及业务操作，只是单纯的做个状态确认)。

### 2.2 数据拆分

### 2.3 不同数据写入Kafka不同的Topic中(dwd层数据)

## 3 具体实现代码清单

### 3.1 封装Kafka工具类

代码见思路文档

### 3.2 封装消费Kafka数据的BaseApp类

### 3.3 DWDLogApp具体实现

#### 识别新老访客

实现思路: 
1.考虑数据的乱序, 使用event-time语义
2.按照mid分组
3.添加5s的滚动窗口
4.使用状态记录首次访问的时间戳
5.如果状态为空, 则此窗口内的最小时间戳的事件为首次访问, 其他均为非首次访问
6.如果状态不为空, 则此窗口内所有的事件均为非首次访问


#### 数据分流

根据日志数据内容,将日志数据分为3类: 页面日志、启动日志和曝光日志。
页面日志输出到主流,启动日志输出到启动侧输出流曝光日志输出到曝光日志侧输出流

#### 不同流写入到Kafka不同Topic

##### 更新MyKafka工具类



# DWD层: 业务数据

业务数据的变化，我们可以通过Maxwell采集到，但是MaxWell是把全部数据统一写入一个Topic中, 这些数据包括业务数据，也包含维度数据，这样显然不利于日后的数据处理，所以这个功能是从Kafka的业务数据ODS层读取数据，经过处理后，将维度数据保存到Hbase，将事实数据写回Kafka作为业务数据的DWD层。

## 1 主要任务

### 1.1 接收Kafka数据，过滤空值数据

### 1.2 实现动态分流功能

由于MaxWell是把全部数据统一写入一个Topic中, 这样显然不利于日后的数据处理。所以需要把各个表拆开处理。但是由于每个表有不同的特点，有些表是维度表，有些表是事实表，有的表既是事实表在某种情况下也是维度表。
在实时计算中一般把维度数据写入存储容器，一般是方便通过主键查询的数据库比如HBase,Redis,MySQL等。
一般把事实数据写入流中，进行进一步处理，最终形成宽表。但是作为Flink实时计算任务，如何得知哪些表是维度表，哪些是事实表呢？而这些表又应该采集哪些字段呢？
这样的配置不适合写在配置文件中，因为这样的话，业务端随着需求变化每增加一张表，就要修改配置重启计算程序。所以这里需要一种动态配置方案，把这种配置长期保存起来，一旦配置有变化，实时计算可以自动感知。
这种可以有两个方案实现:
一种是用Zookeeper存储，通过Watch感知数据变化。
另一种是用mysql数据库存储。
这里选择第二种方案，主要是mysql对于配置数据初始化和维护管理，用sql都比较方便，虽然周期性操作时效性差一点，但是配置变化并不频繁。
所以就有了如下图：

### 1.3 把分好的流保存到对应表、主题中

业务数据保存到Kafka的主题中
维度数据保存到Hbase的表中

## 2 具体实现代码

### 2.1 设计动态配置表



### 2.2 实现思路

​	1.业务数据: mysql->maxwell->Kafka->flink
​	2.动态表配置表的数据: mysql->flink-sql-cdc
​	3.把动态表配置表做成广播流与业务数据进行connect, 从而实现动态控制业务数据的sink方向

### 2.3 读取动态配置表

#### Flink SQL CDC 介绍

CDC 全称是 Change Data Capture ，它是一个比较广义的概念，只要能捕获变更的数据，我们都可以称为 CDC 。业界主要有基于查询的 CDC 和基于日志的 CDC ，可以从下面表格对比他们功能和差异点。

传统的数据同步场景(咱们前面用的场景):
缺点: 采集端组件过多导致维护繁杂

改进后的架构:

Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取全量数据和增量变更数据的 source 组件。目前也已开源，开源地址：https://github.com/ververica/flink-cdc-connectors

修改mysql配置
增加对数据库gmall2021_realtime监控
[mysqld]
server-id= 1
log-bin=mysql-bin
binlog_format=row
binlog-do-db=gmall2021
binlog-do-db=gmall2021_realtime
注意:
1.需要重启mysql数据库
sudo systemctl restart mysqld
2.确认msyql有没有启动成功
ps -ef | grep mysqld
3.注意maxwell不要再采集这个数据库的数据,在maxwell的配置中添加如下配置
filter=exclude:gmall2021_realtime.*
导入CDC依赖
<!-- https://mvnrepository.com/artifact/com.alibaba.ververica/flink-connector-mysql-cdc -->
<dependency>
    <groupId>com.alibaba.ververica</groupId>
    <artifactId>flink-connector-mysql-cdc</artifactId>
    <version>1.1.1</version>
</dependency>
具体实现代码
/**

 * 读取的配置表的数据
    *
 * @param env
 * @return
    */
    private SingleOutputStreamOperator<TableProcess> readProcessTable(StreamExecutionEnvironment env) {
    // 配置表在jdbc, 使用flink-sql-cdc来完成
    final StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
    tenv
        .executeSql("CREATE TABLE `table_process`( " +
                        "   `source_table`  string, " +
                        "   `operate_type`  string, " +
                        "   `sink_type`  string, " +
                        "   `sink_table`  string, " +
                        "   `sink_columns` string, " +
                        "   `sink_pk`  string, " +
                        "   `sink_extend`  string, " +
                        "   PRIMARY KEY (`source_table`,`operate_type`)  NOT ENFORCED" +
                        ")with(" +
                        "   'connector' = 'mysql-cdc', " +
                        "   'hostname' = 'hadoop162', " +
                        "   'port' = '3306', " +
                        "   'username' = 'root', " +
                        "   'password' = 'aaaaaa', " +
                        "   'database-name' = 'gmall2021_realtime', " +
                        "   'table-name' = 'table_process'," +
                        "   'debezium.snapshot.mode' = 'initial' " +  // 读取mysql的全量,增量以及更新数据
                        ")");
    final Table table = tenv.sqlQuery("select " +
                                          "  source_table sourceTable, " +
                                          "  sink_type sinkType, " +
                                          "  operate_type operateType, " +
                                          "  sink_table sinkTable, " +
                                          "  sink_columns sinkColumns, " +
                                          "  sink_pk sinkPk, " +
                                          "  sink_extend sinkExtend " +
                                          "from table_process ");
    return tenv
        .toRetractStream(table, TableProcess.class)
        .filter(t -> t.f0)
        .map(t -> t.f1);
    }
    4.2.4读取业务数据并ETL
    final static OutputTag<Tuple2<JSONObject, TableProcess>> hbaseTag = new OutputTag<Tuple2<JSONObject, TableProcess>>("hbaseTag") {};

public static void main(String[] args) {
    new DWDDbApp().init(2, "DWDDbApp", "ods_db");

}

@Override
public void run(StreamExecutionEnvironment env,
                DataStreamSource<String> sourceStream) {
    // 1. 对流进行etl
    final SingleOutputStreamOperator<JSONObject> etlStream = etl(sourceStream);
    // 2. 读取配置表数据, 使用cdc把数据做成流
    final SingleOutputStreamOperator<TableProcess> processTableStream = readProcessTable(env);

    // 3. 根据配置表数据进行动态分流
    final SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> sink2KafkaStream = dynamicSplit(etlStream, processTableStream);
    final DataStream<Tuple2<JSONObject, TableProcess>> sink2HbaseStream = sink2KafkaStream.getSideOutput(hbaseTag);
    
    // 4. sink数据
    sink2Kafka(sink2KafkaStream);
    sink2Hbase(sink2HbaseStream);

}


/**
 * 对db数据进行etl
    *
 * @param sourceStream
 * @return
    */
    public SingleOutputStreamOperator<JSONObject> etl(DataStreamSource<String> sourceStream) {
    // 根据实际业务, 对数据做一些过滤
    return sourceStream
        .map(JSON::parseObject)
        .filter(value -> value.getString("table") != null
            && value.getJSONObject("data") != null
            && value.getString("data").length() > 3);
    }

4.2.5业务数据表和动态配置表connect
把动态配置表做成广播流, 和数据表流进行connect, 然后进行数据的分流: 事实表数据在主流, hbase数据在侧输出流
/**
 * 对数据流进行动态切分
    *
 * @param dbStream           业务数据流
 * @param processTableStream 动态配置流
 * @return 到kafka的主流
    */
    private SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> dynamicSplit(SingleOutputStreamOperator<JSONObject> dbStream,
                                                                                  SingleOutputStreamOperator<TableProcess> processTableStream) {
    // 1. 把配置表流做成广播状态
    // 1.1 状态描述符:  key: sourceTable_operatorType
    final MapStateDescriptor<String, TableProcess> tableProcessStateDescriptor = new MapStateDescriptor<>(
        "tableProcessState",
        String.class,
        TableProcess.class);
    // 1.2 创建广播流
    final BroadcastStream<TableProcess> tableProcessBroadcastStream = processTableStream
        .broadcast(tableProcessStateDescriptor);
    // 2. 用数据流去关联广播流

    final SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> sinkToKafkaStream = dbStream
        .connect(tableProcessBroadcastStream)
        .process(new BroadcastProcessFunction<JSONObject, TableProcess, Tuple2<JSONObject, TableProcess>>() {

            // 过滤掉不需要的字段
            private void filterColumns(JSONObject dataJsonObj, String sinkColumns) {
                final List<String> columns = Arrays.asList(sinkColumns.split(","));
                dataJsonObj
                    .entrySet()
                    .removeIf(entry -> !columns.contains(entry.getKey()));
            }
        
            @Override
            public void processElement(JSONObject jsonObj,
                                       ReadOnlyContext ctx,
                                       Collector<Tuple2<JSONObject, TableProcess>> out) throws Exception {
                // 处理数据流中的元素
                final ReadOnlyBroadcastState<String, TableProcess> broadcastState = ctx
                    .getBroadcastState(tableProcessStateDescriptor);
                // 事实表数据存入主流(sink 到 Kafka),  维度表数据存入侧输出流(通过Phoenix sink到 HBase)
                // 1. 获取表名,操作类型, 和数据
                String tableName = jsonObj.getString("table");
                String operateType = jsonObj.getString("type");
                JSONObject dataJsonObj = jsonObj.getJSONObject("data");
        
                //1.1 如果是使用Maxwell的初始化功能，那么type类型为bootstrap-insert,我们这里也标记为insert，方便后续处理
                if ("bootstrap-insert".equals(operateType)) {
                    operateType = "insert";
                    jsonObj.put("type", operateType);
                }
                // 2. 获取配置信息
                final String key = tableName + "_" + operateType;
                if (broadcastState.contains(key)) {
                    final TableProcess tableProcess = broadcastState.get(key);
                    // 2.1 sink到Kafka或者HBase的时候, 不是所有字段都需要保存, 过滤掉不需要的
                    filterColumns(dataJsonObj, tableProcess.getSinkColumns());
                    // 2.2 开始分流: 事实表数据在主流, 维度表数据在侧输出流
                    final String sinkType = tableProcess.getSinkType();
                    if (TableProcess.SINK_TYPE_KAFKA.equalsIgnoreCase(sinkType)) {
                        out.collect(Tuple2.of(jsonObj, tableProcess));
                    } else if (TableProcess.SINK_TYPE_HBASE.equalsIgnoreCase(sinkType)) {
                        ctx.output(hbaseTag, Tuple2.of(jsonObj, tableProcess));
                    }
                }
            }
        
            @Override
            public void processBroadcastElement(TableProcess tableProcess,
                                                Context ctx,
                                                Collector<Tuple2<JSONObject, TableProcess>> out) throws Exception {
                System.out.println(tableProcess);
                // 处理广播流中的元素
                // 1. 获取广播状态, 其实存储的是一个map
                final BroadcastState<String, TableProcess> broadcastState = ctx
                    .getBroadcastState(tableProcessStateDescriptor);
                // 2. 以sourceTable作为Key 存入map集合中
                broadcastState.put(tableProcess.getSourceTable() + "_" + tableProcess.getOperateType(), tableProcess);
        
            }
        });

    return sinkToKafkaStream;

}
4.2.6数据sink到正确的位置
Sink到Hbase
导入Phoenix相关依赖
<dependency>
    <groupId>org.apache.phoenix</groupId>
    <artifactId>phoenix-core</artifactId>
    <version>5.0.0-HBase-2.0</version>
    <exclusions>
        <exclusion>
            <groupId>org.glassfish</groupId>
            <artifactId>javax.el</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>30.1-jre</version>
</dependency>
<dependency>
    <groupId>com.google.protobuf</groupId>
    <artifactId>protobuf-java</artifactId>
    <version>2.5.0</version>
</dependency>
具体实现代码
/**
 * 维度表数据写入到Hbase
    *
 * @param sink2HbaseStream 维度表数据流
    */
    private void sink2Hbase(DataStream<Tuple2<JSONObject, TableProcess>> sink2HbaseStream) {
    String phoenixUrl = "jdbc:phoenix:hadoop162,hadoop163,hadoop164:2181";
    sink2HbaseStream
        .keyBy(f -> f.f1.getSinkTable()) // 按照要sink的表进行分组
        .addSink(new RichSinkFunction<Tuple2<JSONObject, TableProcess>>() {
            private Connection conn;
            private ValueState<Boolean> tableCreated;

            @Override
            public void open(Configuration parameters) throws Exception {
                System.out.println("open ...");
                // 建立到Phoenix的连接
                conn = DriverManager.getConnection(phoenixUrl);
                // 定义一个状态, 用来表示该表是否已经被创建
                tableCreated = getRuntimeContext()
                    .getState(new ValueStateDescriptor<Boolean>("tableCreated", Boolean.class));
            }
        
            @Override
            public void invoke(Tuple2<JSONObject, TableProcess> value, Context context) throws Exception {
                System.out.println("invoke ...");
                //1. 检测表是否存在
                checkTable(value);
                //2. 向Phoenix中插入数据
                // upset into user(id,name) values('100', 'lisi')
                write2Hbase(value);
            }
        
            /**
             * 数据写入到Hbase中
             * @param value 要写入的相关数据
             * @throws SQLException
             */
            private void write2Hbase(Tuple2<JSONObject, TableProcess> value) throws SQLException {
                final JSONObject dataJson = value.f0.getJSONObject("data");
                final TableProcess tp = value.f1;
        
                // 2.1 拼接sql语句
                final StringBuilder upsertSql = new StringBuilder();
                upsertSql
                    .append("upsert into ")
                    .append(tp.getSinkTable())
                    .append("(")
                    .append(value.f1.getSinkColumns())
                    .append(") values (");
        
                for (String column : tp.getSinkColumns().split(",")) {
                    upsertSql.append("'").append(dataJson.getString(column)).append("',");
                }
                upsertSql.deleteCharAt(upsertSql.length() - 1);
                upsertSql.append(")");
        
                // 2.2 执行sql语句
                final PreparedStatement ps;
                try {
                    System.out.println(upsertSql);
                    ps = conn.prepareStatement(upsertSql.toString());
                    ps.execute();
                    conn.commit();
                    ps.close();
                } catch (SQLException e) {
                    e.printStackTrace();
                }
        
            }
        
            /**
             * 检测表是否存在, 如果不存在则先在Phoenix中创建表
             * @param value
             * @throws Exception
             */
            private void checkTable(Tuple2<JSONObject, TableProcess> value) throws Exception {
                if (tableCreated.value() == null) {
                    System.out.println("checkTable -> if ");
                    tableCreated.update(true);
                    // 表示第一次插入数据, 需要首先在Phoenix中创建表
                    // 生成建表语句:  create table if not exists user(id varchar, name varchar , constraint pk primary key(id, name) ) SALT_BUCKETS = 3
                    final StringBuilder createSql = new StringBuilder();
                    createSql
                        .append("create table if not exists ")
                        .append(value.f1.getSinkTable())
                        .append("(");
                    for (String column : value.f1.getSinkColumns().split(",")) {
                        createSql.append(column).append(" varchar,");
                    }
                    createSql
                        .append("constraint pk primary key(")
                        .append(value.f1.getSinkPk() == null ? "id" : value.f1.getSinkPk())
                        .append(")");
                    createSql.append(")");
                    createSql.append(value.f1.getSinkExtend() == null ? "" : value.f1.getSinkExtend());
        
                    PreparedStatement ps = conn.prepareStatement(createSql.toString());
                    ps.execute();
        	    conn.commit();
                    ps.close();
        
                }
            }
        
            @Override
            public void close() throws Exception {
                System.out.println("close...");
                if (conn != null) {
                    conn.close();
                }
            }
        });

}

Sink到Kafka
更新MyKafkaUtil
添加新的获取KafkaSink的方法
// 根据内容动态的写入不同的kafka Topic
public static FlinkKafkaProducer<Tuple2<JSONObject, TableProcess>> getKafkaSink() {
    Properties props = new Properties();
    props.setProperty("bootstrap.servers", "hadoop162:9092,hadoop163:9092,hadoop164:9092");
    //如果15分钟没有更新状态，则超时 默认1分钟
    props.setProperty("transaction.timeout.ms", 1000 * 60 * 15 + "");
    return new FlinkKafkaProducer<Tuple2<JSONObject, TableProcess>>(
        "default_topic",
        new KafkaSerializationSchema<Tuple2<JSONObject, TableProcess>>() {
            @Override
            public ProducerRecord<byte[], byte[]> serialize(Tuple2<JSONObject, TableProcess> element, @Nullable Long timestamp) {
                final String topic = element.f1.getSinkTable();
                final JSONObject data = element.f0.getJSONObject("data");

                return new ProducerRecord<>(topic, data.toJSONString().getBytes());
            }
        },
        props,
        FlinkKafkaProducer.Semantic.EXACTLY_ONCE);
}
具体实现代码
/**
 * 事实表数据写入到Kafka
    *
 * @param sink2KafkaStream 事实表数据流
    */
    private void sink2Kafka(SingleOutputStreamOperator<Tuple2<JSONObject, TableProcess>> sink2KafkaStream) {
    sink2KafkaStream.addSink(MyKafkaUtil.getKafkaSink());
    }
    4.3测试上述代码能否正常工作