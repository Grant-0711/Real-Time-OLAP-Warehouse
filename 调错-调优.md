# Flink消费Kafka消费不到

```java
org.apache.kafka.common.errors.TimeoutException:Timeout expired while fetching topic metadata
```

## 原因

kafka地址，zookeeper地址，broker地址等可能配置错误

总之是无法去指定的地址消费数据

# Flink任务提交报错

```shell
org.apache.flink.client.deployment.ClusterRetrieveException: Couldn't retrieve Yarn cluster

bin/flink run -d -t yarn-per-job -c\ com.grant.flink.java.chapter_2.Flink03_WC_UnBoundedStream \
./flink-prepare-1.0-SNAPSHOT.jar
```

## 原因

在flink run 参数指定中，-t后要指定运行模式

yarn-session

yarn-per-job

# idea compile error

```java
java.lang.NoClassDefFoundError: org/apache/flink/runtime/state/StateBackend
```

## 解决

edit configuration 中勾选  including dependencies with scope 'provide'



将配置表制作为流时，流的数据类型封装为java样例类，但是无法直接识别变量

```
Exception in thread "main" org.apache.flink.table.api.TableException: source_table is not found in PojoType<org.hxl.realtime.bean.TableProcess
```

解决

在查询配置表数据时为每一个字段起别名，一一对应

# yarn起不来的问题

容量小于90%时会发生，通过9820web页面查看

## 解决

在停掉所有application之后进入hdfs：/user/grant/.flink

删除相关application_1625023153827_0001等文件

# phoenix.jdbc.PhoenixDriver

java.lang.NoClassDefFoundError: Could not initialize class org.apache.phoenix.jdbc.PhoenixDriver

使用插件：

```xml
<groupId>org.apache.maven.plugins</groupId>
```

```xml
<exclude>org.apache.hadoop:*</exclude>
```

将phoenix依赖改为

```xml
<dependency>
    <groupId>org.apache.phoenix</groupId>
    <artifactId>phoenix-core</artifactId>
    <version>5.0.0-HBase-2.0</version>
</dependency>
```

# PhoenixConfigUtil类找不到或者冲突

phoenix依赖改为

```xml
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-core</artifactId>
            <version>5.0.0-HBase-2.0</version>
            <exclusions>
                <exclusion>
                    <groupId>org.glassfish</groupId>
                    <artifactId>javax.el</artifactId>
                </exclusion>
            </exclusions>
            <scope>provided</scope>
        </dependency>
```

这样打包文件不包含phoenix相关jar，在flink的lib目录下添加相关jar

添加：phoenix-5.0.0-HBase-2.0-client.jar